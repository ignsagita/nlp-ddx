{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0hYelLS4K7V"
      },
      "source": [
        "<h1> Differential Diagnosis with Mistral 7B RAG vs. BioMistral 7B by ContactDoctor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ip-nf71A4HQb",
        "outputId": "8b604919-be3c-487f-aef1-24bc0f040b34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.9/18.9 MB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m303.4/303.4 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m91.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m106.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.9/194.9 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.0/119.0 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m109.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m117.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m91.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m92.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m79.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q streamlit langchain_community chromadb huggingface-hub bitsandbytes pypdf tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wPfBDSg4QUEO"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.makedirs('.streamlit', exist_ok=True)\n",
        "with open('.streamlit/secrets.toml', 'w') as f:\n",
        "    f.write(\"\"\"\n",
        "[huggingface]\n",
        "token = \"secret_token\"\n",
        "\n",
        "[models]\n",
        "rag = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "bio = \"BioMistral/BioMistral-7B\"\n",
        "\"\"\".lstrip())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m8w2TGRlyoyi"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94HxwELP4XyG",
        "outputId": "a218849d-85fb-4c5f-813b-ef688f23bc1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "\n",
        "# IMPORT LIBRARY\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import os\n",
        "import torch\n",
        "\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed # FOR PARALLELIZATION\n",
        "\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain import PromptTemplate\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
        "import threading\n",
        "import time\n",
        "from tenacity import retry, stop_after_attempt, wait_fixed\n",
        "import gc\n",
        "\n",
        "\n",
        "import logging\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional, Dict, Any, List\n",
        "from contextlib import contextmanager\n",
        "import psutil\n",
        "from datetime import datetime\n",
        "\n",
        "# CONFIGURE LOGGING\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "\n",
        "## CODE BLOCK\n",
        "\n",
        "# GENERAL CONFIGURATION\n",
        "PROMPT = \"\"\"Answer the question based only on the following context,:{context}\n",
        "Question:{question}\n",
        "What are the top 10 most likely diagnoses? Be precise, listing one diagnosis per line, and try to cover many unique possibilities.\n",
        "Ensure the order starts with the most likely. The top 10 diagnoses are.\"\"\"\n",
        "MAX_INPUT_TOKENS = 2048 # The sequence length limit of BioMistral-7V\n",
        "MAX_CONTEXT_LENGTH = 4096 # Total context length including prompt\n",
        "DB_DIR = \"./db\"\n",
        "\n",
        "HF_DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "PIPELINE_DEVICE = 0 if torch.cuda.is_available() else -1\n",
        "HF_TOKEN    = st.secrets[\"huggingface\"][\"token\"]\n",
        "model_id    = st.secrets[\"models\"][\"rag\"]\n",
        "bio_model_id= st.secrets[\"models\"][\"bio\"]\n",
        "\n",
        "# using 4bit to save memory for model loading\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "load_in_4bit=True,\n",
        "bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "bnb_4bit_use_double_quant=True,\n",
        "bnb_4bit_quant_type=\"nf4\",\n",
        ")\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    import os\n",
        "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True, max_split_size_mb:512, garbage_collection_threshold:0.8\"\n",
        "\n",
        "# ERROR HANDLING: MESSAGE\n",
        "class ProcessingError(Exception):\n",
        "    \"\"\"Base exception for processing errors\"\"\"\n",
        "    pass\n",
        "\n",
        "class ModelLoadError(ProcessingError):\n",
        "    \"\"\"Error loading or managing models\"\"\"\n",
        "    pass\n",
        "\n",
        "class TokenLimitError(ProcessingError):\n",
        "    \"\"\"Token limit exceeded\"\"\"\n",
        "    pass\n",
        "\n",
        "class GPUMemoryError(ProcessingError):\n",
        "    \"\"\"Memory-related errors\"\"\"\n",
        "    pass\n",
        "\n",
        "class TimeoutError(ProcessingError):\n",
        "    \"\"\"Processing timeout\"\"\"\n",
        "    pass\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### HELPERS TO LOAD THE MODEL ###\n",
        "\n",
        "class ModelManager:\n",
        "    def __init__(self):\n",
        "        self.current_model = None\n",
        "        self.current_model_type = None\n",
        "        self.gpu_lock = threading.Lock()\n",
        "        self.rag_components = None\n",
        "        self.bio_pipeline = None\n",
        "\n",
        "    def check_gpu_memory(self):\n",
        "        \"\"\"Check GPU memory usage\"\"\"\n",
        "        MEMORY_THRESHOLD = 0.9\n",
        "        if torch.cuda.is_available():\n",
        "            memory_used = torch.cuda.memory_allocated()\n",
        "            memory_total = torch.cuda.get_device_properties(0).total_memory\n",
        "            usage_ratio = memory_used / memory_total\n",
        "            logger.info(f\"GPU memory usage: {usage_ratio:.2%}\")\n",
        "\n",
        "            if usage_ratio > MEMORY_THRESHOLD:\n",
        "                logger.warning(f\"High GPU memory usage: {usage_ratio:.2%}\")\n",
        "                return False\n",
        "            return True\n",
        "        return True\n",
        "\n",
        "    def cleanup_memory(self):\n",
        "        \"\"\"Clears CUDA cache and forces garbage collection to save memory\"\"\"\n",
        "        with self.gpu_lock:\n",
        "          if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "            torch.cuda.synchronize()\n",
        "          gc.collect()\n",
        "          logger.info(\"Memory cleanup completed\")\n",
        "\n",
        "    def unload_current_model(self):\n",
        "        \"\"\"Properly unload current model\"\"\"\n",
        "        with self.gpu_lock:\n",
        "            if self.current_model:\n",
        "                try:\n",
        "                    if hasattr(self.current_model, 'model') and hasattr(self.current_model.model, 'to'):\n",
        "                        self.current_model.model.to('cpu')\n",
        "                    del self.current_model\n",
        "                    self.current_model = None\n",
        "                    self.current_model_type = None\n",
        "                    self.cleanup_memory()\n",
        "                    logger.info(\"Model unloaded successfully\")\n",
        "                except Exception as e:\n",
        "                    logger.error(f\"Error unloading model: {e}\")\n",
        "\n",
        "\n",
        "    def get_rag_components(self):\n",
        "        \"\"\"Get RAG components with proper management\"\"\"\n",
        "        if self.rag_components is None:\n",
        "            model = None\n",
        "            pipe = None\n",
        "            try:\n",
        "                logger.info(\"Loading RAG components...\")\n",
        "                embedding = HuggingFaceEmbeddings(\n",
        "                    model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
        "                    model_kwargs={\"device\": HF_DEVICE}\n",
        "                )\n",
        "\n",
        "                vs = Chroma(\n",
        "                    embedding_function=embedding,\n",
        "                    persist_directory=DB_DIR,\n",
        "                )\n",
        "                retriever = vs.as_retriever()\n",
        "\n",
        "                model = AutoModelForCausalLM.from_pretrained(\n",
        "                    model_id,\n",
        "                    use_auth_token=HF_TOKEN,\n",
        "                    device_map='auto' if HF_DEVICE == \"cuda\" else \"cpu\",\n",
        "                    torch_dtype=torch.bfloat16 if HF_DEVICE == \"cuda\" else torch.float32,\n",
        "                    quantization_config=bnb_config\n",
        "                )\n",
        "\n",
        "                pipe = pipeline(\n",
        "                    \"text-generation\",\n",
        "                    model=model,\n",
        "                    tokenizer=AutoTokenizer.from_pretrained(model_id, use_auth_token=HF_TOKEN),\n",
        "                    use_fast=True,\n",
        "                    max_new_tokens=256,\n",
        "                )\n",
        "\n",
        "                if HF_DEVICE == \"cuda\":\n",
        "                    pipe.model.to(\"cuda\")\n",
        "\n",
        "                prompt = PromptTemplate(template=PROMPT, input_variables=[\"context\", \"question\"])\n",
        "                self.rag_components = (pipe, retriever, prompt)\n",
        "                logger.info(\"RAG components loaded successfully\")\n",
        "\n",
        "            except Exception as e:\n",
        "              if pipe is not None:\n",
        "                try:\n",
        "                    if hasattr(pipe, 'model') and HF_DEVICE == \"cuda\":\n",
        "                        pipe.model.to(\"cpu\")\n",
        "                    del pipe\n",
        "                except:\n",
        "                    pass\n",
        "            if model is not None:\n",
        "                try:\n",
        "                    if HF_DEVICE == \"cuda\":\n",
        "                        model.to(\"cpu\")\n",
        "                    del model\n",
        "                except:\n",
        "                    pass\n",
        "            self.cleanup_memory()\n",
        "            logger.error(f\"Error loading RAG components: {e}\")\n",
        "            raise ModelLoadError(f\"Failed to load RAG components: {e}\")\n",
        "\n",
        "        return self.rag_components\n",
        "\n",
        "    def get_bio_pipeline(self):\n",
        "        \"\"\"Get Bio pipeline with proper management\"\"\"\n",
        "        if self.bio_pipeline is None:\n",
        "            try:\n",
        "                logger.info(\"Loading Bio pipeline...\")\n",
        "                with self.gpu_lock:\n",
        "                    bio_model = AutoModelForCausalLM.from_pretrained(\n",
        "                        bio_model_id,\n",
        "                        use_auth_token=HF_TOKEN,\n",
        "                        device_map='auto' if HF_DEVICE == \"cuda\" else \"cpu\",\n",
        "                        torch_dtype=torch.bfloat16 if HF_DEVICE == \"cuda\" else torch.float32,\n",
        "                        quantization_config=bnb_config\n",
        "                    )\n",
        "\n",
        "                    self.bio_pipeline = pipeline(\n",
        "                        \"text-generation\",\n",
        "                        model=bio_model,\n",
        "                        tokenizer=AutoTokenizer.from_pretrained(bio_model_id, use_auth_token=HF_TOKEN),\n",
        "                        use_fast=True,\n",
        "                        max_new_tokens=256,\n",
        "                    )\n",
        "\n",
        "                    if HF_DEVICE == \"cuda\":\n",
        "                        self.bio_pipeline.model.to(\"cuda\")\n",
        "\n",
        "                logger.info(\"Bio pipeline loaded successfully\")\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error loading Bio pipeline: {e}\")\n",
        "                raise ModelLoadError(f\"Failed to load Bio pipeline: {e}\")\n",
        "\n",
        "        return self.bio_pipeline\n",
        "\n",
        "# Initialize model manager\n",
        "model_manager = ModelManager()\n",
        "\n",
        "\n",
        "\n",
        "### CACHING HEAVY RESOURCES ###\n",
        "\n",
        "@st.cache_data(show_spinner=False)\n",
        "def build_vectorstore(uploaded_files, DB_DIR =\"./db\"):\n",
        "    \"\"\"Build vector store with improved error handling\"\"\"\n",
        "    try:\n",
        "        logger.info(f\"Building vector store from {len(uploaded_files)} files\")\n",
        "        uploaded_textbook = \"./uploaded_textbook\"\n",
        "\n",
        "        # Create directories\n",
        "        os.makedirs(uploaded_textbook, exist_ok=True)\n",
        "        os.makedirs(DB_DIR, exist_ok=True)\n",
        "\n",
        "        # Save uploaded files\n",
        "        paths = []\n",
        "        for f in uploaded_files:\n",
        "            path = os.path.join(uploaded_textbook, f.name)\n",
        "            with open(path, \"wb\") as fp:\n",
        "                fp.write(f.getbuffer())\n",
        "            paths.append(path)\n",
        "            logger.info(f\"Saved file: {f.name}\")\n",
        "\n",
        "        # Load documents\n",
        "        docs = []\n",
        "        for pdf_path in paths:\n",
        "            try:\n",
        "                pdf_docs = PyPDFLoader(pdf_path).load()\n",
        "                docs.extend(pdf_docs)\n",
        "                logger.info(f\"Loaded {len(pdf_docs)} pages from {pdf_path}\")\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error loading {pdf_path}: {e}\")\n",
        "                st.error(f\"Error loading {pdf_path}: {e}\")\n",
        "\n",
        "        if not docs:\n",
        "            raise ProcessingError(\"No documents were successfully loaded\")\n",
        "\n",
        "        # Split documents\n",
        "        splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
        "            chunk_size=1000, chunk_overlap=150\n",
        "        )\n",
        "        splits = splitter.split_documents(docs)\n",
        "        logger.info(f\"Created {len(splits)} document chunks\")\n",
        "\n",
        "        # Create vector store\n",
        "        embedding = HuggingFaceEmbeddings(\n",
        "            model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
        "            model_kwargs={\"device\": HF_DEVICE}\n",
        "        )\n",
        "\n",
        "        vs = Chroma.from_documents(\n",
        "            splits,\n",
        "            embedding,\n",
        "            persist_directory=DB_DIR,\n",
        "        )\n",
        "        vs.persist()\n",
        "        logger.info(\"Vector store built and persisted successfully\")\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error building vector store: {e}\")\n",
        "        st.error(f\"Error building vector store: {e}\")\n",
        "        return False\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### HELPERS ###\n",
        "\n",
        "@contextmanager\n",
        "def model_context(model_type):\n",
        "    \"\"\"Context manager for safe model usage\"\"\"\n",
        "    try:\n",
        "        if model_type == \"rag\":\n",
        "            model = model_manager.get_rag_components()[0]  # Get pipeline\n",
        "        elif model_type == \"bio\":\n",
        "            model = model_manager.get_bio_pipeline()\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown model type: {model_type}\")\n",
        "\n",
        "        if HF_DEVICE == \"cuda\":\n",
        "            with model_manager.gpu_lock:\n",
        "                model.model.to(\"cuda\")\n",
        "\n",
        "        yield model\n",
        "\n",
        "    finally:\n",
        "        if HF_DEVICE == \"cuda\":\n",
        "            with model_manager.gpu_lock:\n",
        "                if hasattr(model, 'model'):\n",
        "                    model.model.to(\"cpu\")\n",
        "                model_manager.cleanup_memory()\n",
        "\n",
        "def health_check():\n",
        "    \"\"\"Perform system health check\"\"\"\n",
        "    checks = {\n",
        "        'timestamp': datetime.now().isoformat(),\n",
        "        'gpu_available': torch.cuda.is_available(),\n",
        "        'gpu_memory_ok': model_manager.check_gpu_memory() if torch.cuda.is_available() else True,\n",
        "        'index_exists': os.path.exists(DB_DIR) and bool(os.listdir(DB_DIR)) if os.path.exists(DB_DIR) else False,\n",
        "        'cpu_usage': psutil.cpu_percent(interval=None),\n",
        "        'memory_usage': psutil.virtual_memory().percent\n",
        "    }\n",
        "    logger.info(f\"Health check: {checks}\")\n",
        "    return checks\n",
        "\n",
        "@st.cache_resource(show_spinner=False)\n",
        "def get_tokenizer():\n",
        "    \"\"\"Get tokenizer with caching\"\"\"\n",
        "    try:\n",
        "        return AutoTokenizer.from_pretrained(model_id, use_auth_token=HF_TOKEN)\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error loading tokenizer: {e}\")\n",
        "        raise ModelLoadError(f\"Failed to load tokenizer: {e}\")\n",
        "\n",
        "def check_length(text, tokenizer=None):\n",
        "    \"\"\"Check if text length is within limits\"\"\"\n",
        "    try:\n",
        "        if tokenizer is None:\n",
        "            tokenizer = get_tokenizer()\n",
        "        token_count = len(tokenizer.encode(text))\n",
        "        logger.info(f\"Input token count: {token_count}\")\n",
        "\n",
        "        if token_count > MAX_INPUT_TOKENS:\n",
        "            error_msg = f\"Input is {token_count} tokens, over the {MAX_INPUT_TOKENS}-token limit\"\n",
        "            logger.warning(error_msg)\n",
        "            st.warning(error_msg + \". Please shorten it.\")\n",
        "            raise TokenLimitError(error_msg)\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error checking text length: {e}\")\n",
        "        return False\n",
        "\n",
        "def check_context_length(prompt_text, tokenizer=None):\n",
        "    \"\"\"Check if context length is within limits\"\"\"\n",
        "    try:\n",
        "        if tokenizer is None:\n",
        "            tokenizer = get_tokenizer()\n",
        "        token_count = len(tokenizer.encode(prompt_text))\n",
        "        logger.info(f\"Context token count: {token_count}\")\n",
        "\n",
        "        if token_count > MAX_CONTEXT_LENGTH - 256:\n",
        "            error_msg = f\"Total context + prompt is {token_count} tokens, over the {MAX_CONTEXT_LENGTH}-token limit\"\n",
        "            logger.warning(error_msg)\n",
        "            st.warning(error_msg)\n",
        "            raise TokenLimitError(error_msg)\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error checking context length: {e}\")\n",
        "        return False\n",
        "\n",
        "def safe_invoke(model_or_chain, *args, **kwargs):\n",
        "    \"\"\"Safely invoke model with error handling\"\"\"\n",
        "    try:\n",
        "        if hasattr(model_or_chain, \"invoke\"):\n",
        "            result = model_or_chain.invoke(*args, **kwargs)\n",
        "        else:\n",
        "            result = model_or_chain(*args, **kwargs)\n",
        "        return result\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Model invocation error: {e}\")\n",
        "        st.error(f\"MODEL ERROR: {e}\")\n",
        "        raise ProcessingError(f\"Model invocation failed: {e}\")\n",
        "\n",
        "\n",
        "#error handling for bio and naive model\n",
        "@retry(stop=stop_after_attempt(2), wait=wait_fixed(2))\n",
        "def run_rag_pipeline(prompt_text):\n",
        "    \"\"\"Run RAG pipeline with improved error handling\"\"\"\n",
        "    try:\n",
        "        logger.info(\"Running RAG pipeline\")\n",
        "        with model_context(\"rag\") as pipe:\n",
        "            raw_output = safe_invoke(pipe, prompt_text, max_new_tokens=256)\n",
        "\n",
        "            if raw_output and len(raw_output) > 0:\n",
        "                raw = raw_output[0][\"generated_text\"]\n",
        "                output = raw[len(prompt_text):].lstrip() if raw.startswith(prompt_text) else raw\n",
        "                logger.info(\"RAG pipeline completed successfully\")\n",
        "                return output\n",
        "            else:\n",
        "                raise ProcessingError(\"No output generated from RAG pipeline\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"RAG pipeline error: {e}\")\n",
        "        raise ProcessingError(f\"RAG pipeline failed: {str(e)}\")\n",
        "\n",
        "@retry(stop=stop_after_attempt(2), wait=wait_fixed(2))\n",
        "def run_bio_pipeline(prompt_text):\n",
        "    \"\"\"Run Bio pipeline with improved error handling\"\"\"\n",
        "    try:\n",
        "        logger.info(\"Running Bio pipeline\")\n",
        "        with model_context(\"bio\") as pipe:\n",
        "            raw_output = safe_invoke(pipe, prompt_text, max_new_tokens=256)\n",
        "\n",
        "            if raw_output and len(raw_output) > 0:\n",
        "                raw = raw_output[0][\"generated_text\"]\n",
        "                output = raw[len(prompt_text):].lstrip() if raw.startswith(prompt_text) else raw\n",
        "                logger.info(\"Bio pipeline completed successfully\")\n",
        "                return output\n",
        "            else:\n",
        "                raise ProcessingError(\"No output generated from Bio pipeline\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Bio pipeline error: {e}\")\n",
        "        raise ProcessingError(f\"Bio pipeline failed: {str(e)}\")\n",
        "\n",
        "\n",
        "#case processing\n",
        "def process_case(txt, use_rag=True, use_bio=True):\n",
        "    \"\"\"Process a single case with comprehensive error handling\"\"\"\n",
        "    results = {\"Case\": txt[:100] + \"...\" if len(txt) > 100 else txt}\n",
        "    logger.info(f\"Processing case of length {len(txt)}\")\n",
        "\n",
        "    try:\n",
        "        tokenizer = get_tokenizer()\n",
        "\n",
        "        # Validate input length\n",
        "        if not check_length(txt, tokenizer):\n",
        "            error_msg = \"Case too long, exceeds token limit\"\n",
        "            results[\"Mistral7B+RAG\"] = error_msg\n",
        "            results[\"BioMistral7B\"] = error_msg\n",
        "            return results\n",
        "\n",
        "        # Process with RAG\n",
        "        if use_rag:\n",
        "            try:\n",
        "                logger.info(\"Starting RAG processing\")\n",
        "                _, retriever, prompt_template = model_manager.get_rag_components()\n",
        "\n",
        "                # Retrieve context\n",
        "                docs = retriever.get_relevant_documents(txt)\n",
        "                context = \"\\n\\n\".join(d.page_content for d in docs)\n",
        "\n",
        "                # Generate prompt\n",
        "                prompt_text = prompt_template.format_prompt(context=context, question=txt).to_string()\n",
        "\n",
        "                # Check context length and truncate if needed\n",
        "                try:\n",
        "                    check_context_length(prompt_text, tokenizer)\n",
        "                except TokenLimitError:\n",
        "                    logger.warning(\"Context too long, truncating\")\n",
        "                    context_shortened = context[:len(context)//2] + \"...\"\n",
        "                    prompt_text = prompt_template.format_prompt(context=context_shortened, question=txt).to_string()\n",
        "\n",
        "                # Run RAG pipeline\n",
        "                rag_out = run_rag_pipeline(prompt_text)\n",
        "                results[\"Mistral7B+RAG\"] = rag_out\n",
        "                logger.info(\"RAG processing completed\")\n",
        "\n",
        "            except Exception as e:\n",
        "                error_msg = f\"RAG ERROR: {str(e)}\"\n",
        "                logger.error(error_msg)\n",
        "                results[\"Mistral7B+RAG\"] = error_msg\n",
        "        else:\n",
        "            results[\"Mistral7B+RAG\"] = \"RAG processing skipped\"\n",
        "\n",
        "        # Process with Bio model\n",
        "        if use_bio:\n",
        "            try:\n",
        "                logger.info(\"Starting Bio processing\")\n",
        "                bio_prompt = PROMPT.format(context=\"\", question=txt)\n",
        "                bio_out = run_bio_pipeline(bio_prompt)\n",
        "                results[\"BioMistral7B\"] = bio_out\n",
        "                logger.info(\"Bio processing completed\")\n",
        "\n",
        "            except Exception as e:\n",
        "                error_msg = f\"BIO ERROR: {str(e)}\"\n",
        "                logger.error(error_msg)\n",
        "                results[\"BioMistral7B\"] = error_msg\n",
        "        else:\n",
        "            results[\"BioMistral7B\"] = \"Bio processing skipped\"\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"GENERAL ERROR: {str(e)}\"\n",
        "        logger.error(error_msg)\n",
        "        results[\"Mistral7B+RAG\"] = error_msg\n",
        "        results[\"BioMistral7B\"] = error_msg\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "\n",
        "### STREAMLIT UI ###\n",
        "\n",
        "st.title(\"Differential Diagnosis: Mistral 7B RAG vs. Bio Mistral 7B by BioMistral\")\n",
        "st.caption(\"Helps the doctor/nurse to develop their differential diagnosis using LLM models\")\n",
        "st.caption(\"Pretrained Medical Textbook: ....\")\n",
        "\n",
        "\n",
        "# Additional files\n",
        "with st.sidebar:\n",
        "    st.header(\"Upload additional resources for RAG (type:.pdf)\")\n",
        "    UploadedFiles = st.file_uploader(\"Upload here and click on 'Upload'\", type=\"pdf\", accept_multiple_files=True)\n",
        "    MAX_LINES = 3 # limit maximum document uploaded\n",
        "    if len(UploadedFiles) > MAX_LINES:\n",
        "      st.warning(f\"Maximum number of files reached. Only the first {MAX_LINES} will be processed.\")\n",
        "      UploadedFiles = UploadedFiles[:MAX_LINES]\n",
        "\n",
        "\n",
        "    if st.button(\"Build Index\"):\n",
        "        if not UploadedFiles:\n",
        "            st.error(\"Select at least one PDF first.\")\n",
        "        else:\n",
        "            with st.spinner(\"Indexing…\"):\n",
        "                build_vectorstore(UploadedFiles, DB_DIR)\n",
        "            st.success(\"RAG index is ready!\")\n",
        "\n",
        "    st.markdown(\"---\")\n",
        "    st.header(\"System Status\")\n",
        "    if st.button(\"Check System Health\"):\n",
        "        health = health_check()\n",
        "        for key, value in health.items():\n",
        "            if key == 'timestamp':\n",
        "                st.text(f\"Last check: {value}\")\n",
        "            elif isinstance(value, bool):\n",
        "                st.text(f\"{key}: {'✅' if value else '❌'}\")\n",
        "            elif isinstance(value, (int, float)):\n",
        "                st.text(f\"{key}: {value:.1f}%\")\n",
        "            else:\n",
        "                st.text(f\"{key}: {value}\")\n",
        "\n",
        "    if st.button(\"Clear GPU Memory\"):\n",
        "        model_manager.cleanup_memory()\n",
        "        st.success(\"GPU memory cleared!\")\n",
        "\n",
        "    st.markdown(\"---\")\n",
        "    st.header(\"Batch processing case upload (type:.xlsx)\")\n",
        "    excel_file = st.file_uploader(\n",
        "        \"Upload .XLSX\",\n",
        "        type=\"xlsx\",\n",
        "        accept_multiple_files=False)\n",
        "\n",
        "### SINGLE CASE ###\n",
        "\n",
        "st.subheader(\"SINGLE CASE\")\n",
        "question = st.text_area(\"Case Narrative:\",\n",
        "                        height=180,\n",
        "                        placeholder=\"For example: 22-year-old patient with TB was admitted to hospital today. The patient has been to a country outside Sweden. The patient came back to Sweden from the other country. The patient has had a fever for two weeks and is admitted. The doctor has prescribed a medicine. \")\n",
        "st.write(f\"The number of characters are {len(question)} characters.\")\n",
        "\n",
        "\n",
        "\n",
        "if st.button('Start Processing'):\n",
        "    # Check system health\n",
        "    health = health_check()\n",
        "    if not health['index_exists']:\n",
        "        st.error(\"Please upload and build your PDF index first!\")\n",
        "        st.stop()\n",
        "\n",
        "    if not health['gpu_memory_ok']:\n",
        "        st.warning(\"High GPU memory usage detected. Processing may be slower.\")\n",
        "\n",
        "    try:\n",
        "        check_length(question)\n",
        "        with st.spinner(\"Processing...\"):\n",
        "            result = process_case(question)\n",
        "\n",
        "        tabs = st.tabs([\"BIOMode\", \"RAGMode\", \"System Info\"])\n",
        "\n",
        "        with tabs[0]:\n",
        "            st.markdown(\"**BioMistral 7B**\")\n",
        "            st.text(result['BioMistral7B'])\n",
        "\n",
        "        with tabs[1]:\n",
        "            st.markdown(\"**Mistral 7B + RAG**\")\n",
        "            st.text(result['Mistral7B+RAG'])\n",
        "\n",
        "        with tabs[2]:\n",
        "            st.markdown(\"**System Status**\")\n",
        "            st.json(health_check())\n",
        "\n",
        "    except TokenLimitError as e:\n",
        "        st.error(f\"Input too long: {e}\")\n",
        "    except Exception as e:\n",
        "        st.error(f\"Processing error: {e}\")\n",
        "        logger.error(f\"UI processing error: {e}\")\n",
        "\n",
        "### BATCH PROCESSING ###\n",
        "\n",
        "def process_batch_safely(cases, progress_bar=None):\n",
        "    \"\"\"Process batch of cases with proper error handling\"\"\"\n",
        "    BATCH_SIZE = 5\n",
        "    results = []\n",
        "    total_cases = len(cases)\n",
        "    logger.info(f\"Starting batch processing of {total_cases} cases\")\n",
        "\n",
        "    # Process in smaller batches to manage memory\n",
        "    for i in range(0, total_cases, BATCH_SIZE):\n",
        "        batch = cases[i:i+BATCH_SIZE]\n",
        "        logger.info(f\"Processing batch {i//BATCH_SIZE + 1}\")\n",
        "\n",
        "        for case_idx, case in enumerate(batch):\n",
        "            try:\n",
        "                # Add validation\n",
        "                if not case or not case.strip():\n",
        "                    logger.warning(f\"Skipping empty case {i + case_idx + 1}\")\n",
        "                    error_result = {\n",
        "                        \"Case\": \"Empty case\",\n",
        "                        \"Mistral7B+RAG\": \"ERROR: Empty case provided\",\n",
        "                        \"BioMistral7B\": \"ERROR: Empty case provided\"\n",
        "                    }\n",
        "                    results.append(error_result)\n",
        "                    if progress_bar:\n",
        "                        progress_bar.progress((i + case_idx + 1) / total_cases)\n",
        "                    continue\n",
        "\n",
        "                # Check system health before processing\n",
        "                health = health_check()\n",
        "                if not health['gpu_memory_ok']:\n",
        "                    logger.warning(\"GPU memory high, forcing cleanup\")\n",
        "                    model_manager.cleanup_memory()\n",
        "\n",
        "                result = process_case(case)\n",
        "                results.append(result)\n",
        "\n",
        "                # Update progress bar if provided\n",
        "                if progress_bar:\n",
        "                    progress_bar.progress((i + case_idx + 1) / total_cases)\n",
        "\n",
        "                logger.info(f\"Completed case {i + case_idx + 1}/{total_cases}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                error_result = {\n",
        "                    \"Case\": case[:100] + \"...\" if len(case) > 100 else case,\n",
        "                    \"Mistral7B+RAG\": f\"ERROR: {str(e)}\",\n",
        "                    \"BioMistral7B\": f\"ERROR: {str(e)}\"\n",
        "                }\n",
        "                results.append(error_result)\n",
        "\n",
        "                if progress_bar:\n",
        "                    progress_bar.progress((i + case_idx + 1) / total_cases)\n",
        "\n",
        "                logger.error(f\"Error processing case {i + case_idx + 1}: {e}\")\n",
        "\n",
        "        # Cleanup between batches\n",
        "        model_manager.cleanup_memory()\n",
        "        logger.info(f\"Completed batch {i//BATCH_SIZE + 1}\")\n",
        "\n",
        "    logger.info(f\"Batch processing completed. Processed {len(results)} cases\")\n",
        "    return results\n",
        "\n",
        "\n",
        "\n",
        "st.markdown(\"---\")\n",
        "st.subheader(\"BATCH MODE\")\n",
        "\n",
        "if excel_file:\n",
        "    df = pd.read_excel(excel_file)\n",
        "\n",
        "    if st.button(\"Start Batch Processing\"):\n",
        "      health = health_check()\n",
        "      if not health['index_exists']:\n",
        "          st.error(\"Please upload and build your PDF index first!\")\n",
        "          st.stop()\n",
        "\n",
        "      if not health['gpu_memory_ok']:\n",
        "          st.warning(\"High GPU memory usage detected. Batch processing may be slower.\")\n",
        "\n",
        "      try:\n",
        "          if 'Case' not in df.columns:\n",
        "            st.error(\"Excel file must contain a 'Case' column!\")\n",
        "            st.stop()\n",
        "          cases = df[\"Case\"].tolist()\n",
        "          prog = st.progress(0)\n",
        "\n",
        "          # Use the defined batch processing function\n",
        "          results = process_batch_safely(cases, prog)\n",
        "\n",
        "      try:\n",
        "          out_df = pd.DataFrame(results)\n",
        "          st.download_button(\n",
        "              \"Download Results as CSV\",\n",
        "              data=out_df.to_csv(index=False),\n",
        "              file_name=\"ddx_comparison.csv\",\n",
        "              mime=\"text/csv\"\n",
        "          )\n",
        "          logger.info(\"CSV download prepared successfully\")\n",
        "      except Exception as e:\n",
        "          st.error(f\"Error preparing CSV download: {e}\")\n",
        "          logger.error(f\"CSV generation error: {e}\")\n",
        "\n",
        "          # Show sample of results\n",
        "          st.write(\"Sample of processed results:\")\n",
        "          st.dataframe(out_df.head())\n",
        "\n",
        "          logger.info(\"Batch processing completed\")\n",
        "\n",
        "      except pd.errors.EmptyDataError:\n",
        "          st.error(\"Excel file is empty!\")\n",
        "      except pd.errors.ParserError:\n",
        "          st.error(\"Error reading Excel file. Please check the format.\")\n",
        "      except KeyError as e:\n",
        "          st.error(f\"Missing column in Excel file: {e}\")\n",
        "      except Exception as e:\n",
        "          st.error(f\"Batch processing error: {e}\")\n",
        "          logger.error(f\"Batch processing error: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VpnZ57j8jEzI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8UdR9UqO6XCQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-g4_J0UfB6mQ"
      },
      "source": [
        "<h2>Install local-tunnel </h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73Wu2XhiCEiV",
        "outputId": "dd0dbf23-c5ba-45c5-f4ed-a876afe8a979"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K\n",
            "added 22 packages in 3s\n",
            "\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K\n",
            "\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K3 packages are looking for funding\n",
            "\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K  run `npm fund` for details\n",
            "\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K"
          ]
        }
      ],
      "source": [
        "!npm install localtunnel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ubFGRkLPwZW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EPWz5NtCJnR"
      },
      "source": [
        "<h2> Run Streamlit in background </h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hAdTKrDCJM9",
        "outputId": "f27f1e0a-6681-4c59-8c27-143cc4434e13"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "35.187.241.161\n",
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0Kyour url is: https://tired-groups-clap.loca.lt\n"
          ]
        }
      ],
      "source": [
        "# AND Expose to the port 8501\n",
        "!streamlit run /content/app.py &>/content/logs.txt & npx localtunnel --port 8501 & curl ipv4.icanhazip.com"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CmWloMc4F7W"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
