{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0hYelLS4K7V"
      },
      "source": [
        "<h1> Differential Diagnosis with Llama 3-8B RAG vs. BioLlama 3 by ContactDoctor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ip-nf71A4HQb",
        "outputId": "4394ba98-3e8a-4f1c-dcae-43f79d7700f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m96.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m84.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m87.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m84.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m74.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m87.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.4/188.4 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.3/65.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.0/119.0 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m97.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m805.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m85.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m90.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q streamlit langchain_community chromadb huggingface-hub bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wPfBDSg4QUEO"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.makedirs('.streamlit', exist_ok=True)\n",
        "with open('.streamlit/secrets.toml', 'w') as f:\n",
        "    f.write(\"\"\"\n",
        "[huggingface]\n",
        "token = \"secret_token\"\n",
        "\n",
        "[models]\n",
        "rag = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "bio = \"ContactDoctor/Bio-Medical-Llama-3-8B\"\n",
        "\"\"\".lstrip())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94HxwELP4XyG",
        "outputId": "a5e05749-63e3-422c-e6ac-3933935d18d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "\n",
        "# IMPORT LIBRARY\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import os\n",
        "import torch\n",
        "\n",
        "# FOR PARALLELIZATION\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "#from langchain_community.embeddings import OllamaEmbeddings\n",
        "#from langchain_community.llms import Ollama\n",
        "#from langchain_core.runnables import RunnablePassthrough\n",
        "#from langchain_core.output_parsers import StrOutputParser\n",
        "#from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "#from langchain_core.messages import AIMessage, HumanMessage\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
        "#from langchain import HuggingFacePipeline, LLMChain\n",
        "from langchain import PromptTemplate\n",
        "\n",
        "from transformers import AutoConfig\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# CODE BLOCK\n",
        "\n",
        "PROMPT = \"\"\"Answer the question based only on the following context,:{context}\n",
        "Question:{question}\n",
        "What are the top 10 most likely diagnoses? Be precise, listing one diagnosis per line, and try to cover many unique possibilities.\n",
        "Ensure the order starts with the most likely. The top 10 diagnoses are.\"\"\"\n",
        "MAX_INPUT_TOKENS = 2048 # The sequence length limit of BioMistral-7V\n",
        "DB_DIR = \"./db\"\n",
        "\n",
        "HF_DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "PIPELINE_DEVICE = 0 if torch.cuda.is_available() else -1\n",
        "HF_TOKEN    = st.secrets[\"huggingface\"][\"token\"]\n",
        "model_id    = st.secrets[\"models\"][\"rag\"]\n",
        "bio_model_id= st.secrets[\"models\"][\"bio\"]\n",
        "\n",
        "\n",
        "### CACHING HEAVY RESOURCES ###\n",
        "\n",
        "@st.cache_resource(show_spinner=False)\n",
        "def get_embedding_fn():\n",
        "  return HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\", model_kwargs = {\"device\": HF_DEVICE})\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Load Llama-3-8B RAG\n",
        "\n",
        "@st.cache_resource(show_spinner=False)\n",
        "def get_rag_components():\n",
        "    embed_fn = get_embedding_fn()\n",
        "    vect  = Chroma(persist_directory=DB_DIR,\n",
        "                   embedding_function=embed_fn)\n",
        "    retriever  = vect.as_retriever()\n",
        "\n",
        "    # quantization_config = BitsAndBytesConfig(\n",
        "    # load_in_8bit=True,\n",
        "    # llm_int8_threshold=6.0,\n",
        "    # ) if HF_DEVICE == \"cuda\" else None\n",
        "\n",
        "    config = AutoConfig.from_pretrained(model_id)\n",
        "\n",
        "    # If we have CUDA, do 8-bit quantization; otherwise load normally\n",
        "    kwargs = {\n",
        "        \"use_auth_token\": HF_TOKEN,\n",
        "        \"device_map\": {\"\": HF_DEVICE},\n",
        "        # torch_dtype only really matters on GPU\n",
        "        \"torch_dtype\": torch.bfloat16 if HF_DEVICE==\"cuda\" else torch.float32,\n",
        "        \"offload_folder\": \"offload\",\n",
        "        \"offload_state_dict\": True,\n",
        "    }\n",
        "\n",
        "    if HF_DEVICE == \"cuda\":\n",
        "        kwargs[\"load_in_8bit\"] = True\n",
        "\n",
        "    mod = AutoModelForCausalLM.from_pretrained(model_id, **kwargs)\n",
        "\n",
        "\n",
        "    pipe  = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=mod,\n",
        "        tokenizer=AutoTokenizer.from_pretrained(model_id, use_auth_token=HF_TOKEN),\n",
        "        use_fast=True,\n",
        "        max_new_tokens=128,\n",
        "    )\n",
        "\n",
        "    prompt = PromptTemplate(template=PROMPT, input_variables=[\"context\", \"question\"])\n",
        "    return pipe, retriever, prompt\n",
        "\n",
        "\n",
        "# Load Bio Model\n",
        "\n",
        "\n",
        "@st.cache_resource(show_spinner=False)\n",
        "def get_bio_pipeline():\n",
        "\n",
        "    # If we have CUDA, do 8-bit quantization; otherwise load normally\n",
        "    kwargs = {\n",
        "        \"use_auth_token\": HF_TOKEN,\n",
        "        \"device_map\": {\"\": \"cpu\"},\n",
        "        # torch_dtype only really matters on GPU\n",
        "        \"torch_dtype\": torch.bfloat16 if HF_DEVICE==\"cuda\" else torch.float32,\n",
        "        \"offload_folder\": {\"\",\"offload\"},\n",
        "        \"offload_state_dict\": {\"\", True},\n",
        "    }\n",
        "\n",
        "    if HF_DEVICE == \"cuda\":\n",
        "        kwargs[\"load_in_8bit\"] = True\n",
        "\n",
        "    bio_mod     = AutoModelForCausalLM.from_pretrained(bio_model_id, **kwargs)\n",
        "\n",
        "    bio_pipe  =  pipeline(\n",
        "        \"text-generation\",\n",
        "        model=bio_mod,\n",
        "        tokenizer=AutoTokenizer.from_pretrained(bio_model_id, use_auth_token=HF_TOKEN),\n",
        "        use_fast=True,\n",
        "        max_new_tokens=128,\n",
        "    )\n",
        "\n",
        "    return bio_pipe\n",
        "\n",
        "@st.cache_resource\n",
        "def get_tokenizer():\n",
        "    return AutoTokenizer.from_pretrained(model_id, use_auth_token=HF_TOKEN)\n",
        "\n",
        "\n",
        "# Tricks for Expensive File I/O: streamlit caching\n",
        "@st.cache_data(show_spinner=False)\n",
        "def build_vectorstore(uploaded_files):\n",
        "    os.makedirs(\"UploadedTextbook\", exist_ok=True)\n",
        "    paths = []\n",
        "    for f in uploaded_files:\n",
        "        path = os.path.join(\"UploadedTextbook\", f.name)\n",
        "        with open(path, \"wb\") as fp:\n",
        "            fp.write(f.getbuffer())\n",
        "        paths.append(path)\n",
        "\n",
        "    docs = []\n",
        "    for pdf in paths:\n",
        "        docs.extend(PyPDFLoader(pdf).load())\n",
        "    splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
        "        chunk_size=1500, chunk_overlap=200\n",
        "    )\n",
        "    splits = splitter.split_documents(docs)\n",
        "\n",
        "    vs = Chroma.from_documents(\n",
        "        documents=splits,\n",
        "        embedding_function=get_embedding_fn(),\n",
        "        persist_directory=DB_DIR,\n",
        "    )\n",
        "    vs.persist()\n",
        "    return True\n",
        "\n",
        "\n",
        "### HELPERS ###\n",
        "# Make sure the token inputs are within the limit\n",
        "def check_length(text):\n",
        "    tok = get_tokenizer()\n",
        "    token_count = len(tok.encode(text))\n",
        "    if token_count > MAX_INPUT_TOKENS:\n",
        "        st.warning(f\"Your input is {token_count} tokens, over the {MAX_INPUT_TOKENS}-token limit. Please shorten it.\")\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "def safe_invoke(model_or_chain, *args, **kwargs):\n",
        "    try:\n",
        "        if hasattr(model_or_chain, \"invoke\"):\n",
        "            return model_or_chain.invoke(*args, **kwargs)\n",
        "        return model_or_chain(*args, **kwargs)\n",
        "\n",
        "    except Exception as e:\n",
        "        st.error(f\"MODEL ERROR: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def process_case(txt):\n",
        "    # 1) Retrieve context\n",
        "    rag_pipe, rag_retriever, prompt = get_rag_components()\n",
        "    docs    = rag_retriever.get_relevant_documents(txt)\n",
        "    context = \"\\n\\n\".join(d.page_content for d in docs)\n",
        "\n",
        "    # 2) Generate with RAG‐LLM\n",
        "    prompt_text = prompt.format_prompt(context=context, question=txt).to_string()\n",
        "    rag_out_raw = safe_invoke(rag_pipe, prompt_text, max_new_tokens=128)\n",
        "    if rag_out_raw:\n",
        "        raw = rag_out_raw[0][\"generated_text\"]\n",
        "        # strip prompt echo\n",
        "        rag_out = raw[len(prompt_text):].lstrip() if raw.startswith(prompt_text) else raw\n",
        "    else:\n",
        "        rag_out = \"Error\"\n",
        "\n",
        "    # 3) Generate with Bio‐LLM\n",
        "    bio_pipe    = get_bio_pipeline()\n",
        "    bio_prompt  = PROMPT.format(context=\"\", question=txt)\n",
        "    bio_raw = safe_invoke(bio_pipe, bio_prompt, max_new_tokens=128)\n",
        "    if bio_raw:\n",
        "        raw_b = bio_raw[0][\"generated_text\"]\n",
        "        bio_out = raw_b[len(bio_prompt):].lstrip() if raw_b.startswith(bio_prompt) else raw_b\n",
        "    else:\n",
        "        bio_out = \"Error\"\n",
        "\n",
        "    return {\"Case\": txt, \"Llama3+RAG\": rag_out, \"BioLlama3\": bio_out}\n",
        "\n",
        "\n",
        "\n",
        "# Then when generating, temporarily move to GPU:\n",
        "def generate_bio(bio_pipe, prompt):\n",
        "    # move to GPU\n",
        "    bio_pipe.model.to(\"cuda\")\n",
        "    out = bio_pipe(prompt)[0][\"generated_text\"]\n",
        "    # move back to CPU to free VRAM\n",
        "    bio_pipe.model.to(\"cpu\")\n",
        "    return out\n",
        "\n",
        "\n",
        "\n",
        "### STREAMLIT UI ###\n",
        "st.title(\"Differential Diagnosis: Llama 3-8B RAG vs. BioLlama 3 by ContactDoctor\")\n",
        "st.caption(\"Predefined Medical Guidelines for RAG: Kumar & Clark's Clinical Medicine 10th Ed. 2020\")\n",
        "\n",
        "# Additional files\n",
        "with st.sidebar:\n",
        "    st.header(\"Upload additional resources for RAG (type:.pdf)\")\n",
        "    UploadedFiles = st.file_uploader(\"Upload here and click on 'Upload'\", type=\"pdf\", accept_multiple_files=True)\n",
        "\n",
        "    if st.button(\"Build Index\"):\n",
        "        try:\n",
        "            os.mkdir(\"UploadedTextbook\")\n",
        "            if not UploadedFiles:\n",
        "              st.error('Select at least one PDF file.')\n",
        "            else:\n",
        "              with st.spinner(\"Indexing…\"):\n",
        "                  build_vectorstore(UploadedFiles)\n",
        "              st.success(\"RAG index is ready!\")\n",
        "        except:\n",
        "            print(\"File already exists\")\n",
        "\n",
        "    st.markdown(\"---\")\n",
        "    st.header(\"Batch processing case upload (type:.csv)\")\n",
        "    csv_file = st.file_uploader(\n",
        "        \"Upload CSV\",\n",
        "        type=\"csv\",\n",
        "        accept_multiple_files=False)\n",
        "\n",
        "### SINGLE CASE ###\n",
        "st.subheader(\"SINGLE CASE\")\n",
        "question = st.text_area(\"Case Narrative:\",\n",
        "                        height=180,\n",
        "                        placeholder=\"For example: 22-year-old patient with TB was admitted to hospital today. The patient has been to a country outside Sweden. The patient came back to Sweden from the other country. The patient has had a fever for two weeks and is admitted. The doctor has prescribed a medicine. \")\n",
        "st.write(f\"The number of characters are {len(question)} characters.\")\n",
        "\n",
        "if st.button('Start Processing'):\n",
        "  if check_length(question):\n",
        "    with st.spinner(\"Processing...\"):\n",
        "      rag_pipe, rag_retriever, prompt = get_rag_components()\n",
        "      docs = rag_retriever.get_relevant_documents(question)\n",
        "      context = \"\\n\\n\".join([d.page_content for d in docs])\n",
        "      prompt_text = prompt.format_prompt(context=context, question=question).to_string()\n",
        "\n",
        "      st.markdown(\"**Llama 3-8B + RAG**\")\n",
        "      raw = rag_pipe(prompt_text)[0][\"generated_text\"]\n",
        "      # remove the prompt echo\n",
        "      if raw.startswith(prompt_text):\n",
        "          answer = raw[len(prompt_text):].lstrip()\n",
        "      else:\n",
        "          answer = raw\n",
        "      st.text(answer)\n",
        "\n",
        "      bio_pipe = get_bio_pipeline()\n",
        "      bio_prompt = PROMPT.format(context=\"\",question=question)\n",
        "      raw_bio    = bio_pipe(bio_prompt)[0][\"generated_text\"]\n",
        "\n",
        "      # strip off the echoed prompt if present\n",
        "      if raw_bio.startswith(bio_prompt):\n",
        "          bio_output = raw_bio[len(bio_prompt):].lstrip()\n",
        "      else:\n",
        "          bio_output = raw_bio\n",
        "\n",
        "      st.markdown(\"**BioLlama 3**\")\n",
        "      st.text(bio_output)\n",
        "\n",
        "  else:\n",
        "    st.stop()\n",
        "\n",
        "### BATCH PROCESSING ###\n",
        "st.markdown(\"---\")\n",
        "st.subheader(\"BATCH MODE\")\n",
        "\n",
        "if csv_file:\n",
        "    df = pd.read_csv(csv_file)\n",
        "    if st.button(\"Start Batch Processing\"):\n",
        "        results = []\n",
        "        futures = []\n",
        "        prog = st.progress(0)\n",
        "\n",
        "\n",
        "        with ThreadPoolExecutor(max_workers=4) as exe:\n",
        "            for txt in df[\"Case\"]:\n",
        "                futures.append(exe.submit(process_case, txt))\n",
        "\n",
        "            # as each case completes, update progress\n",
        "            for i, fut in enumerate(as_completed(futures)):\n",
        "                results.append(fut.result())\n",
        "                prog.progress((i + 1) / len(futures))\n",
        "\n",
        "        out_df = pd.DataFrame(results)\n",
        "        st.download_button(\n",
        "            \"Download Results as CSV\",\n",
        "            data=out_df.to_csv(index=False),\n",
        "            file_name=\"ddx_comparison.csv\"\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "8UdR9UqO6XCQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-g4_J0UfB6mQ"
      },
      "source": [
        "<h2>Install local-tunnel </h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73Wu2XhiCEiV",
        "outputId": "e7f176fe-f446-4439-de09-a37d174c9900"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K\n",
            "up to date, audited 23 packages in 792ms\n",
            "\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K\n",
            "\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K3 packages are looking for funding\n",
            "\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K  run `npm fund` for details\n",
            "\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K\n",
            "2 \u001b[31m\u001b[1mhigh\u001b[22m\u001b[39m severity vulnerabilities\n",
            "\n",
            "To address all issues (including breaking changes), run:\n",
            "  npm audit fix --force\n",
            "\n",
            "Run `npm audit` for details.\n",
            "\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K"
          ]
        }
      ],
      "source": [
        "!npm install localtunnel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EPWz5NtCJnR"
      },
      "source": [
        "<h2> Run Streamlit in background </h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hAdTKrDCJM9",
        "outputId": "3b39c37e-9eec-4847-cf5e-179d9d1f020d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "34.82.189.63\n",
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0Kyour url is: https://tough-monkeys-lose.loca.lt\n"
          ]
        }
      ],
      "source": [
        "# AND Expose to the port 8501\n",
        "!streamlit run /content/app.py &>/content/logs.txt & npx localtunnel --port 8501 & curl ipv4.icanhazip.com"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CmWloMc4F7W"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
