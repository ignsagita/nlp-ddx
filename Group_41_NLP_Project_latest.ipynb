{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0hYelLS4K7V"
      },
      "source": [
        "<h1> Differential Diagnosis with Llama 3-8B RAG vs. BioLlama 3 by ContactDoctor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ip-nf71A4HQb",
        "outputId": "20831046-e25e-4382-9a19-3ff40e85b083"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m67.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m68.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.9/18.9 MB\u001b[0m \u001b[31m57.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m84.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.4/188.4 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.3/65.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.0/119.0 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m100.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m75.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m541.2/664.8 MB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:04\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q streamlit langchain_community chromadb huggingface-hub bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wPfBDSg4QUEO"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.makedirs('.streamlit', exist_ok=True)\n",
        "with open('.streamlit/secrets.toml', 'w') as f:\n",
        "    f.write(\"\"\"\n",
        "[huggingface]\n",
        "token = \"secret_token\"\n",
        "\n",
        "[models]\n",
        "rag = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "bio = \"ContactDoctor/Bio-Medical-Llama-3-8B\"\n",
        "\"\"\".lstrip())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94HxwELP4XyG"
      },
      "outputs": [],
      "source": [
        "%%writefile app.py\n",
        "\n",
        "# IMPORT LIBRARY\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import os\n",
        "import torch\n",
        "\n",
        "# FOR PARALLELIZATION\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain import PromptTemplate\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
        "import threading\n",
        "import time\n",
        "from tenacity import retry, stop_after_attempt, wait_fixed\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# CODE BLOCK\n",
        "\n",
        "PROMPT = \"\"\"Answer the question based only on the following context,:{context}\n",
        "Question:{question}\n",
        "What are the top 10 most likely diagnoses? Be precise, listing one diagnosis per line, and try to cover many unique possibilities.\n",
        "Ensure the order starts with the most likely. The top 10 diagnoses are.\"\"\"\n",
        "MAX_INPUT_TOKENS = 2048 # The sequence length limit of BioMistral-7V\n",
        "DB_DIR = \"./db\"\n",
        "\n",
        "HF_DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "PIPELINE_DEVICE = 0 if torch.cuda.is_available() else -1\n",
        "HF_TOKEN    = st.secrets[\"huggingface\"][\"token\"]\n",
        "model_id    = st.secrets[\"models\"][\"rag\"]\n",
        "bio_model_id= st.secrets[\"models\"][\"bio\"]\n",
        "\n",
        "### HELPERS TO LOAD THE MODEL ###\n",
        "#lock to serialize any “move‐model‐on/off GPU” calls\n",
        "gpu_lock = threading.Lock()\n",
        "\n",
        "@retry(stop=stop_after_attempt(3), wait=wait_fixed(5))\n",
        "def hf_from_pretrained(*args, **kwargs):\n",
        "    return AutoModelForCausalLM.from_pretrained(*args, **kwargs)\n",
        "\n",
        "\n",
        "### CACHING HEAVY RESOURCES ###\n",
        "\n",
        "@st.cache_resource(show_spinner=False)\n",
        "def get_embedding_fn():\n",
        "  return HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\", model_kwargs = {\"device\": HF_DEVICE})\n",
        "\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "load_in_8bit=True,\n",
        "bnb_8bit_quant_type=\"nf4\",\n",
        "llm_int8_threshold=6.0,\n",
        "llm_int8_enable_fp32_cpu_offload=True,\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# Load Llama-3-8B RAG\n",
        "@st.cache_resource(show_spinner=False)\n",
        "def get_rag_components():\n",
        "    embed_fn = get_embedding_fn()\n",
        "    vect  = Chroma(persist_directory=DB_DIR,\n",
        "                   embedding_function=embed_fn)\n",
        "    retriever  = vect.as_retriever()\n",
        "\n",
        "    # If we have CUDA, do 8-bit quantization; otherwise load normally\n",
        "    kwargs = {\n",
        "        \"use_auth_token\": HF_TOKEN,\n",
        "        \"device_map\": \"auto\",\n",
        "        # torch_dtype only really matters on GPU\n",
        "        \"torch_dtype\": torch.bfloat16 if HF_DEVICE==\"cuda\" else torch.float32,\n",
        "        \"quantization_config\": bnb_config,\n",
        "    }\n",
        "\n",
        "    if HF_DEVICE == \"cuda\":\n",
        "        kwargs[\"load_in_8bit\"] = True\n",
        "\n",
        "    mod = hf_from_pretrained(model_id, **kwargs)\n",
        "\n",
        "\n",
        "    pipe  = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=mod,\n",
        "        tokenizer=AutoTokenizer.from_pretrained(model_id, use_auth_token=HF_TOKEN),\n",
        "        device=PIPELINE_DEVICE,\n",
        "        use_fast=True,\n",
        "        max_length=512,\n",
        "    )\n",
        "\n",
        "    prompt = PromptTemplate(template=PROMPT, input_variables=[\"context\", \"question\"])\n",
        "    return pipe, retriever, prompt\n",
        "\n",
        "\n",
        "# Load Bio Model\n",
        "@st.cache_resource(show_spinner=False)\n",
        "def get_bio_pipeline():\n",
        "\n",
        "    # If we have CUDA, do 8-bit quantization; otherwise load normally\n",
        "    kwargs = {\n",
        "        \"use_auth_token\": HF_TOKEN,\n",
        "        \"device_map\": \"auto\",\n",
        "        # torch_dtype only really matters on GPU\n",
        "        \"torch_dtype\": torch.bfloat16 if HF_DEVICE==\"cuda\" else torch.float32,\n",
        "        \"quantization_config\": bnb_config,\n",
        "    }\n",
        "\n",
        "    if HF_DEVICE == \"cuda\":\n",
        "        kwargs[\"load_in_8bit\"] = True\n",
        "\n",
        "    bio_mod     = hf_from_pretrained(bio_model_id, **kwargs)\n",
        "\n",
        "    bio_pipe  =  pipeline(\n",
        "        \"text-generation\",\n",
        "        model=bio_mod,\n",
        "        tokenizer=AutoTokenizer.from_pretrained(bio_model_id, use_auth_token=HF_TOKEN),\n",
        "        device=PIPELINE_DEVICE,\n",
        "        use_fast=True,\n",
        "        max_length=512,\n",
        "    )\n",
        "\n",
        "    return bio_pipe\n",
        "\n",
        "@st.cache_resource\n",
        "def get_tokenizer():\n",
        "    return AutoTokenizer.from_pretrained(model_id, use_auth_token=HF_TOKEN)\n",
        "\n",
        "\n",
        "# Tricks for Expensive File I/O: streamlit caching\n",
        "@st.cache_data(show_spinner=False)\n",
        "def build_vectorstore(uploaded_files):\n",
        "    os.makedirs(\"UploadedTextbook\", exist_ok=True)\n",
        "    paths = []\n",
        "    for f in uploaded_files:\n",
        "        path = os.path.join(\"UploadedTextbook\", f.name)\n",
        "        with open(path, \"wb\") as fp:\n",
        "            fp.write(f.getbuffer())\n",
        "        paths.append(path)\n",
        "\n",
        "    docs = []\n",
        "    for pdf in paths:\n",
        "        docs.extend(PyPDFLoader(pdf).load())\n",
        "    splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
        "        chunk_size=1500, chunk_overlap=200\n",
        "    )\n",
        "    splits = splitter.split_documents(docs)\n",
        "\n",
        "    vs = Chroma.from_documents(\n",
        "        documents=splits,\n",
        "        embedding_function=get_embedding_fn(),\n",
        "        persist_directory=DB_DIR,\n",
        "    )\n",
        "    vs.persist()\n",
        "    return True\n",
        "\n",
        "\n",
        "### HELPERS TO PROCESS THE OUTPUT ###\n",
        "# Make sure the token inputs are within the limit\n",
        "def check_length(text):\n",
        "    tok = get_tokenizer()\n",
        "    token_count = len(tok.encode(text))\n",
        "    if token_count > MAX_INPUT_TOKENS:\n",
        "        st.warning(f\"Your input is {token_count} tokens, over the {MAX_INPUT_TOKENS}-token limit. Please shorten it.\")\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "def safe_invoke(model_or_chain, *args, **kwargs):\n",
        "    try:\n",
        "        if hasattr(model_or_chain, \"invoke\"):\n",
        "            return model_or_chain.invoke(*args, **kwargs)\n",
        "        return model_or_chain(*args, **kwargs)\n",
        "\n",
        "    except Exception as e:\n",
        "        st.error(f\"MODEL ERROR: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def process_case(txt):\n",
        "    # 1) Retrieve context\n",
        "    try:\n",
        "      rag_pipe, rag_retriever, prompt = get_rag_components()\n",
        "      docs    = rag_retriever.get_relevant_documents(txt)\n",
        "      context = \"\\n\\n\".join(d.page_content for d in docs)\n",
        "\n",
        "      # 2) Generate with RAG‐LLM\n",
        "      prompt_text = prompt.format_prompt(context=context, question=txt).to_string()\n",
        "      rag_out_raw = safe_invoke(rag_pipe, prompt_text, max_new_tokens=128)\n",
        "      if rag_out_raw:\n",
        "          raw = rag_out_raw[0][\"generated_text\"]\n",
        "          # strip prompt echo\n",
        "          rag_out = raw[len(prompt_text):].lstrip() if raw.startswith(prompt_text) else raw\n",
        "      else:\n",
        "          rag_out = \"Error\"\n",
        "\n",
        "      # 3) Generate with Bio‐LLM\n",
        "      bio_pipe    = get_bio_pipeline()\n",
        "      bio_prompt  = PROMPT.format(context=\"\", question=txt)\n",
        "      bio_raw = safe_invoke(bio_pipe, bio_prompt, max_new_tokens=128)\n",
        "      if bio_raw:\n",
        "          raw_b = bio_raw[0][\"generated_text\"]\n",
        "          bio_out = raw_b[len(bio_prompt):].lstrip() if raw_b.startswith(bio_prompt) else raw_b\n",
        "      else:\n",
        "          bio_out = \"Error\"\n",
        "\n",
        "      return {\"Case\": txt, \"Llama3+RAG\": rag_out, \"BioLlama3\": bio_out}\n",
        "\n",
        "    except Exception as e:\n",
        "        return {\"Case\": txt, \"Llama3+RAG\": f\"ERROR: {e}\", \"BioLlama3\": \"\"}\n",
        "\n",
        "# Then when generating, temporarily move to GPU:\n",
        "def generate_bio(bio_pipe, prompt):\n",
        "  with gpu_lock:\n",
        "    # move to GPU\n",
        "    bio_pipe.model.to(\"cuda\")\n",
        "    out = bio_pipe(prompt)[0][\"generated_text\"]\n",
        "    # move back to CPU to free VRAM\n",
        "    bio_pipe.model.to(\"cpu\")\n",
        "  return out\n",
        "\n",
        "\n",
        "\n",
        "### STREAMLIT UI ###\n",
        "st.title(\"Differential Diagnosis: Llama 3-8B RAG vs. BioLlama 3 by ContactDoctor\")\n",
        "st.caption(\"Predefined Medical Guidelines for RAG: Kumar & Clark's Clinical Medicine 10th Ed. 2020\")\n",
        "\n",
        "# Additional files\n",
        "with st.sidebar:\n",
        "    st.header(\"Upload additional resources for RAG (type:.pdf)\")\n",
        "    UploadedFiles = st.file_uploader(\"Upload here and click on 'Upload'\", type=\"pdf\", accept_multiple_files=True)\n",
        "\n",
        "    if st.button(\"Build Index\"):\n",
        "        try:\n",
        "          run_id = str(time.time())\n",
        "          if not UploadedFiles:\n",
        "              st.error(\"Select at least one PDF first.\")\n",
        "          else:\n",
        "              st.session_state[\"building_index\"] = True\n",
        "\n",
        "          if st.session_state.get(\"building_index\"):\n",
        "            with st.spinner(\"Indexing…\"):\n",
        "                build_vectorstore(UploadedFiles, run_id)\n",
        "            st.success(\"RAG index is ready!\")\n",
        "            st.session_state[\"building_index\"] = False\n",
        "\n",
        "        except:\n",
        "            print(\"File already exists\")\n",
        "\n",
        "    st.markdown(\"---\")\n",
        "    st.header(\"Batch processing case upload (type:.csv)\")\n",
        "    csv_file = st.file_uploader(\n",
        "        \"Upload CSV\",\n",
        "        type=\"csv\",\n",
        "        accept_multiple_files=False)\n",
        "\n",
        "### SINGLE CASE ###\n",
        "st.subheader(\"SINGLE CASE\")\n",
        "question = st.text_area(\"Case Narrative:\",\n",
        "                        height=180,\n",
        "                        placeholder=\"For example: 22-year-old patient with TB was admitted to hospital today. The patient has been to a country outside Sweden. The patient came back to Sweden from the other country. The patient has had a fever for two weeks and is admitted. The doctor has prescribed a medicine. \")\n",
        "st.write(f\"The number of characters are {len(question)} characters.\")\n",
        "\n",
        "if st.button('Start Processing'):\n",
        "  if check_length(question):\n",
        "    with st.spinner(\"Processing...\"):\n",
        "\n",
        "      tabs = st.tabs([\"BIOMode\", \"RAGMode\"])\n",
        "\n",
        "      with tabs[0]:\n",
        "        #BioLlama-3 execution\n",
        "        bio_pipe = get_bio_pipeline()\n",
        "        bio_prompt = PROMPT.format(context=\"\",question=question)\n",
        "        raw_bio    = bio_pipe(bio_prompt)[0][\"generated_text\"]\n",
        "\n",
        "        #remove the prompt echo\n",
        "        if raw_bio.startswith(bio_prompt):\n",
        "            bio_output = raw_bio[len(bio_prompt):].lstrip()\n",
        "        else:\n",
        "            bio_output = raw_bio\n",
        "\n",
        "        st.markdown(\"**BioLlama 3**\")\n",
        "        st.text(bio_output)\n",
        "\n",
        "        #free up memory for RAG\n",
        "        del bio_pipe\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "      with tabs[1]:\n",
        "        if not os.path.isdir(DB_DIR) or not os.listdir(DB_DIR):\n",
        "          st.error(\"Please upload and build your PDF index first!\")\n",
        "          st.stop()\n",
        "\n",
        "        rag_pipe, rag_retriever, prompt = get_rag_components()\n",
        "        docs = rag_retriever.get_relevant_documents(question)\n",
        "        context = \"\\n\\n\".join([d.page_content for d in docs])\n",
        "        prompt_text = prompt.format_prompt(context=context, question=question).to_string()\n",
        "\n",
        "        st.markdown(\"**Llama 3-8B + RAG**\")\n",
        "        raw = rag_pipe(prompt_text)[0][\"generated_text\"]\n",
        "\n",
        "        #remove the prompt echo\n",
        "        if raw.startswith(prompt_text):\n",
        "            answer = raw[len(prompt_text):].lstrip()\n",
        "        else:\n",
        "            answer = raw\n",
        "        st.text(answer)\n",
        "\n",
        "        #free up memory from RAG\n",
        "        del rag_pipe, rag_retriever, prompt\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "  else:\n",
        "    st.stop()\n",
        "\n",
        "### BATCH PROCESSING ###\n",
        "st.markdown(\"---\")\n",
        "st.subheader(\"BATCH MODE\")\n",
        "\n",
        "if csv_file:\n",
        "    df = pd.read_csv(csv_file)\n",
        "    if st.button(\"Start Batch Processing\"):\n",
        "        results = []\n",
        "        futures = []\n",
        "        prog = st.progress(0)\n",
        "\n",
        "        BATCH_WORKERS = 1 if HF_DEVICE != \"cuda\" else 2\n",
        "        with ThreadPoolExecutor(max_workers=BATCH_WORKERS) as exe:\n",
        "            for txt in df[\"Case\"]:\n",
        "                futures.append(exe.submit(process_case, txt))\n",
        "\n",
        "            # as each case completes, update progress\n",
        "            for i, fut in enumerate(as_completed(futures)):\n",
        "                results.append(fut.result())\n",
        "                prog.progress((i + 1) / len(futures))\n",
        "\n",
        "        out_df = pd.DataFrame(results)\n",
        "        st.download_button(\n",
        "            \"Download Results as CSV\",\n",
        "            data=out_df.to_csv(index=False),\n",
        "            file_name=\"ddx_comparison.csv\"\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8UdR9UqO6XCQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-g4_J0UfB6mQ"
      },
      "source": [
        "<h2>Install local-tunnel </h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73Wu2XhiCEiV"
      },
      "outputs": [],
      "source": [
        "!npm install localtunnel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EPWz5NtCJnR"
      },
      "source": [
        "<h2> Run Streamlit in background </h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_hAdTKrDCJM9"
      },
      "outputs": [],
      "source": [
        "# AND Expose to the port 8501\n",
        "!streamlit run /content/app.py &>/content/logs.txt & npx localtunnel --port 8501 & curl ipv4.icanhazip.com"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CmWloMc4F7W"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
