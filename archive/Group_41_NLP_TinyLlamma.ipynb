{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0hYelLS4K7V"
      },
      "source": [
        "<h1> Differential Diagnosis with Tiny Llama RAG vs. BioLlama 3 by ContactDoctor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ip-nf71A4HQb",
        "outputId": "f2f259ad-1fe8-449c-e73e-4b9efb9d268c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.9/18.9 MB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.3/302.3 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.4/188.4 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.3/65.3 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.0/119.0 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m74.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m64.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q streamlit langchain_community chromadb huggingface-hub bitsandbytes pypdf tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wPfBDSg4QUEO"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.makedirs('.streamlit', exist_ok=True)\n",
        "with open('.streamlit/secrets.toml', 'w') as f:\n",
        "    f.write(\"\"\"\n",
        "[huggingface]\n",
        "token = \"secret_token\"\n",
        "\n",
        "[models]\n",
        "rag = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "bio = \"ContactDoctor/Bio-Medical-Llama-3-8B\"\n",
        "\"\"\".lstrip())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94HxwELP4XyG",
        "outputId": "fb7486bb-6269-455a-83e8-9f27034fd890"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "\n",
        "# IMPORT LIBRARY\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import os\n",
        "import torch\n",
        "import re\n",
        "\n",
        "# FOR PARALLELIZATION\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain import PromptTemplate\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
        "import threading\n",
        "import time\n",
        "from tenacity import retry, stop_after_attempt, wait_fixed\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# CODE BLOCK\n",
        "\n",
        "PROMPT = \"\"\"Answer the question based only on the following context,:{context}\n",
        "Question:{question}\n",
        "What are the top 10 most likely diagnoses? Be precise, listing one diagnosis per line, and try to cover many unique possibilities.\n",
        "Ensure the order starts with the most likely. The top 10 diagnoses are.\"\"\"\n",
        "MAX_INPUT_TOKENS = 2048 # The sequence length limit of BioMistral-7V\n",
        "DB_DIR = \"./db_im\"\n",
        "\n",
        "HF_DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "PIPELINE_DEVICE = 0 if torch.cuda.is_available() else -1\n",
        "HF_TOKEN    = st.secrets[\"huggingface\"][\"token\"]\n",
        "model_id    = st.secrets[\"models\"][\"rag\"]\n",
        "bio_model_id= st.secrets[\"models\"][\"bio\"]\n",
        "\n",
        "\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "load_in_8bit=True,\n",
        "bnb_8bit_quant_type=\"nf4\",\n",
        "llm_int8_threshold=6.0,\n",
        "llm_int8_enable_fp32_cpu_offload=True,\n",
        ")\n",
        "\n",
        "### HELPERS TO LOAD THE MODEL ###\n",
        "#lock to serialize any “move‐model‐on/off GPU” calls\n",
        "gpu_lock = threading.Lock()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def safe_invoke(model_or_chain, *args, **kwargs):\n",
        "    try:\n",
        "        if hasattr(model_or_chain, \"invoke\"):\n",
        "            return model_or_chain.invoke(*args, **kwargs)\n",
        "        return model_or_chain(*args, **kwargs)\n",
        "\n",
        "    except Exception as e:\n",
        "        st.error(f\"MODEL ERROR: {e}\")\n",
        "        return None\n",
        "\n",
        "def choose_specialty(current_case, pipe, prompt_specialty):\n",
        "    \"\"\" This function chooses the specialty that a medical case belongs to automatically.\n",
        "    It takes a model (model instance) and the case (str).\n",
        "    The model = Ollama(model=\"mistral\") was used in testing. It returns\n",
        "    the directory of the vector database as a string\"\"\"\n",
        "\n",
        "    prompt_specialty = prompt_specialty.format_prompt(current_case=current_case).to_string()\n",
        "\n",
        "    response_specialty = safe_invoke(pipe, prompt_specialty, max_new_tokens=128)\n",
        "\n",
        "    raw_specialty = response_specialty[0][\"generated_text\"]\n",
        "    # strip prompt echo\n",
        "    specialty_out = raw_specialty[len(prompt_specialty):].lstrip() if raw_specialty.startswith(prompt_specialty) else raw_specialty\n",
        "\n",
        "    db_dict = {'internal medicine': './db_im', 'obstetrics and gynecology': './db_og', 'pediatrics':'./db_p','surgery':'./db_surg','psychiatry':'./db_psy'}\n",
        "\n",
        "    result_specialty = re.sub(r'\\d\\.', '', specialty_out).strip().lower()\n",
        "    return db_dict[result_specialty]\n",
        "\n",
        "### CACHING HEAVY RESOURCES ###\n",
        "\n",
        "@st.cache_resource(show_spinner=False)\n",
        "def get_embedding_fn():\n",
        "  return HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\", model_kwargs = {\"device\": HF_DEVICE})\n",
        "\n",
        "\n",
        "\n",
        "# Load Tiny Llama RAG\n",
        "@st.cache_resource(show_spinner=False)\n",
        "def get_rag_components(txt):\n",
        "\n",
        "\n",
        "\n",
        "    mod = AutoModelForCausalLM.from_pretrained(model_id,\n",
        "                                               use_auth_token= HF_TOKEN,\n",
        "                                               device_map='auto',\n",
        "                                               torch_dtype= torch.bfloat16 if HF_DEVICE==\"cuda\" else torch.float32,\n",
        "                                               quantization_config=bnb_config)\n",
        "\n",
        "\n",
        "    pipe  = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=mod,\n",
        "        tokenizer=AutoTokenizer.from_pretrained(model_id, use_auth_token=HF_TOKEN),\n",
        "        #device=PIPELINE_DEVICE,\n",
        "        use_fast=True,\n",
        "        max_new_tokens=256,\n",
        "        do_sample=True,   # Often helpful for smaller models\n",
        "        temperature=0.7,  # Adjust for TinyLlama\n",
        "        top_p=0.9,        # Adjust for TinyLlama\n",
        "    )\n",
        "\n",
        "    prompt = PromptTemplate(template=PROMPT, input_variables=[\"context\", \"question\"])\n",
        "\n",
        "    PROMPT_specialty_template = \"\"\"{current_case} Do not explain. What is the medical specialty of this case? Choose from this list 1. Internal Medicine, 2. Obstetrics and Gynecology, 3. Pediatrics, 4. Surgery 5. Psychiatry\"\"\"\n",
        "\n",
        "\n",
        "    prompt_specialty = PromptTemplate(template=PROMPT_specialty_template, input_variables=[\"current_case\"])\n",
        "\n",
        "    DB_DIR = choose_specialty(txt, pipe, prompt_specialty)\n",
        "\n",
        "\n",
        "    vs = Chroma(\n",
        "        embedding_function=get_embedding_fn(),\n",
        "        persist_directory=DB_DIR,\n",
        "    )\n",
        "\n",
        "    retriever  = vs.as_retriever()\n",
        "    return pipe, retriever, prompt\n",
        "\n",
        "\n",
        "# Load Bio Model\n",
        "@st.cache_resource(show_spinner=False)\n",
        "def get_bio_pipeline():\n",
        "\n",
        "    bio_mod     = AutoModelForCausalLM.from_pretrained(bio_model_id,\n",
        "                                               use_auth_token= HF_TOKEN,\n",
        "                                               device_map='auto',\n",
        "                                               torch_dtype= torch.bfloat16 if HF_DEVICE==\"cuda\" else torch.float32,\n",
        "                                               quantization_config=bnb_config)\n",
        "\n",
        "    bio_pipe  =  pipeline(\n",
        "        \"text-generation\",\n",
        "        model=bio_mod,\n",
        "        tokenizer=AutoTokenizer.from_pretrained(bio_model_id, use_auth_token=HF_TOKEN),\n",
        "        #device=PIPELINE_DEVICE,\n",
        "        use_fast=True,\n",
        "        max_new_tokens=256,\n",
        "    )\n",
        "\n",
        "    return bio_pipe\n",
        "\n",
        "@st.cache_resource\n",
        "def get_tokenizer():\n",
        "    return AutoTokenizer.from_pretrained(model_id, use_auth_token=HF_TOKEN)\n",
        "\n",
        "\n",
        "# Tricks for Expensive File I/O: streamlit caching\n",
        "@st.cache_data(show_spinner=False)\n",
        "def build_vectorstore(uploaded_files, persist_directory):\n",
        "    folder_dict = {'./db_im': 'im_folder', './db_og': 'og_folder', './db_p':'p_folder','./db_surg':'surg_folder','./db_psy':'psy_folder'}\n",
        "\n",
        "    UploadedTextbook = folder_dict[persist_directory]\n",
        "\n",
        "    os.makedirs(UploadedTextbook, exist_ok=True)\n",
        "    paths = []\n",
        "    for f in uploaded_files:\n",
        "        path = os.path.join(UploadedTextbook, f.name)\n",
        "        with open(path, \"wb\") as fp:\n",
        "            fp.write(f.getbuffer())\n",
        "        paths.append(path)\n",
        "\n",
        "    docs = []\n",
        "    for pdf in paths:\n",
        "        docs.extend(PyPDFLoader(pdf).load())\n",
        "    splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
        "        chunk_size=1500, chunk_overlap=200\n",
        "    )\n",
        "    splits = splitter.split_documents(docs)\n",
        "\n",
        "    vs = Chroma.from_documents(\n",
        "        splits,\n",
        "        get_embedding_fn(),\n",
        "        persist_directory=persist_directory,\n",
        "    )\n",
        "    vs.persist()\n",
        "    return True\n",
        "\n",
        "\n",
        "### HELPERS TO PROCESS THE OUTPUT ###\n",
        "# Make sure the token inputs are within the limit\n",
        "def check_length(text):\n",
        "    tok = get_tokenizer()\n",
        "    token_count = len(tok.encode(text))\n",
        "    if token_count > MAX_INPUT_TOKENS:\n",
        "        st.warning(f\"Your input is {token_count} tokens, over the {MAX_INPUT_TOKENS}-token limit. Please shorten it.\")\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "\n",
        "\n",
        "def process_case(txt):\n",
        "    # 1) Retrieve context\n",
        "    try:\n",
        "      rag_pipe, rag_retriever, prompt = get_rag_components(txt)\n",
        "      docs    = rag_retriever.get_relevant_documents(txt)\n",
        "      context = \"\\n\\n\".join(d.page_content for d in docs)\n",
        "\n",
        "      # 2) Generate with RAG‐LLM\n",
        "      prompt_text = prompt.format_prompt(context=context, question=txt).to_string()\n",
        "      rag_out_raw = safe_invoke(rag_pipe, prompt_text, max_new_tokens=128)\n",
        "      if rag_out_raw:\n",
        "          raw = rag_out_raw[0][\"generated_text\"]\n",
        "          # strip prompt echo\n",
        "          rag_out = raw[len(prompt_text):].lstrip() if raw.startswith(prompt_text) else raw\n",
        "      else:\n",
        "          rag_out = \"Error\"\n",
        "\n",
        "      # 3) Generate with Bio‐LLM\n",
        "      bio_pipe    = get_bio_pipeline()\n",
        "      bio_prompt  = PROMPT.format(context=\"\", question=txt)\n",
        "      bio_raw = safe_invoke(bio_pipe, bio_prompt, max_new_tokens=128)\n",
        "      if bio_raw:\n",
        "          raw_b = bio_raw[0][\"generated_text\"]\n",
        "          bio_out = raw_b[len(bio_prompt):].lstrip() if raw_b.startswith(bio_prompt) else raw_b\n",
        "      else:\n",
        "          bio_out = \"Error\"\n",
        "\n",
        "      return {\"Case\": txt, \"TinyLlama+RAG\": rag_out, \"BioLlama3\": bio_out}\n",
        "\n",
        "    except Exception as e:\n",
        "        return {\"Case\": txt, \"TinyLlama+RAG\": f\"ERROR: {e}\", \"BioLlama3\": \"\"}\n",
        "\n",
        "# Then when generating, temporarily move to GPU:\n",
        "def generate_bio(bio_pipe, prompt):\n",
        "  with gpu_lock:\n",
        "    # move to GPU\n",
        "    bio_pipe.model.to(\"cuda\")\n",
        "    out = bio_pipe(prompt)[0][\"generated_text\"]\n",
        "    # move back to CPU to free VRAM\n",
        "    bio_pipe.model.to(\"cpu\")\n",
        "  return out\n",
        "\n",
        "\n",
        "\n",
        "### STREAMLIT UI ###\n",
        "st.title(\"Differential Diagnosis: TinyLlama RAG vs. BioLlama 3 by ContactDoctor\")\n",
        "st.caption(\"Helps the doctor/nurse to develop their differential diagnosis using LLM models\")\n",
        "\n",
        "# Additional files\n",
        "with st.sidebar:\n",
        "    db_dict = {'internal medicine': './db_im', 'obstetrics and gynecology': './db_og', 'pediatrics':'./db_p','surgery':'./db_surg','psychiatry':'./db_psy'}\n",
        "    persist_directory = \"./db_im\"\n",
        "    specialty = st.selectbox(\"Choose Specialty\", ('internal medicine', 'obstetrics and gynecology', 'pediatrics','surgery','psychiatry'))\n",
        "    persist_directory = db_dict[specialty]\n",
        "    # if st.button(\"Set Specialty\"):\n",
        "    #     persist_directory = db_dict[specialty]\n",
        "    #     print(persist_directory)\n",
        "\n",
        "    st.header(\"Upload additional resources for RAG (type:.pdf)\")\n",
        "    UploadedFiles = st.file_uploader(\"Upload here and click on 'Upload'\", type=\"pdf\", accept_multiple_files=True)\n",
        "\n",
        "    if st.button(\"Build Index\"):\n",
        "        if not UploadedFiles:\n",
        "            st.error(\"Select at least one PDF first.\")\n",
        "        else:\n",
        "            with st.spinner(\"Indexing…\"):\n",
        "                build_vectorstore(UploadedFiles, persist_directory)\n",
        "            st.success(\"RAG index is ready!\")\n",
        "\n",
        "    st.markdown(\"---\")\n",
        "    st.header(\"Batch processing case upload (type:.csv)\")\n",
        "    csv_file = st.file_uploader(\n",
        "        \"Upload CSV\",\n",
        "        type=\"csv\",\n",
        "        accept_multiple_files=False)\n",
        "\n",
        "### SINGLE CASE ###\n",
        "st.subheader(\"SINGLE CASE\")\n",
        "question = st.text_area(\"Case Narrative:\",\n",
        "                        height=180,\n",
        "                        placeholder=\"For example: 22-year-old patient with TB was admitted to hospital today. The patient has been to a country outside Sweden. The patient came back to Sweden from the other country. The patient has had a fever for two weeks and is admitted. The doctor has prescribed a medicine. \")\n",
        "st.write(f\"The number of characters are {len(question)} characters.\")\n",
        "\n",
        "if st.button('Start Processing'):\n",
        "  if check_length(question):\n",
        "    with st.spinner(\"Processing...\"):\n",
        "\n",
        "      tabs = st.tabs([\"BIOMode\", \"RAGMode\"])\n",
        "\n",
        "      with tabs[0]:\n",
        "        #BioLlama-3 execution\n",
        "        bio_pipe = get_bio_pipeline()\n",
        "        bio_prompt = PROMPT.format(context=\"\",question=question)\n",
        "        raw_bio    = bio_pipe(bio_prompt)[0][\"generated_text\"]\n",
        "\n",
        "        #remove the prompt echo\n",
        "        if raw_bio.startswith(bio_prompt):\n",
        "            bio_output = raw_bio[len(bio_prompt):].lstrip()\n",
        "        else:\n",
        "            bio_output = raw_bio\n",
        "\n",
        "        st.markdown(\"**BioLlama 3**\")\n",
        "        st.text(bio_output)\n",
        "\n",
        "        #free up memory for RAG\n",
        "        del bio_pipe\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "      with tabs[1]:\n",
        "        if not os.path.isdir(DB_DIR) or not os.listdir(DB_DIR):\n",
        "          st.error(\"Please upload and build your PDF index first!\")\n",
        "          st.stop()\n",
        "\n",
        "        rag_pipe, rag_retriever, prompt = get_rag_components(question)\n",
        "        docs = rag_retriever.get_relevant_documents(question)\n",
        "        context = \"\\n\\n\".join([d.page_content for d in docs])\n",
        "        prompt_text = prompt.format_prompt(context=context, question=question).to_string()\n",
        "\n",
        "        st.markdown(\"**TinyLlama + RAG**\")\n",
        "        raw = rag_pipe(prompt_text)[0][\"generated_text\"]\n",
        "\n",
        "        #remove the prompt echo\n",
        "        if raw.startswith(prompt_text):\n",
        "            answer = raw[len(prompt_text):].lstrip()\n",
        "        else:\n",
        "            answer = raw\n",
        "        st.text(answer)\n",
        "\n",
        "        #free up memory from RAG\n",
        "        del rag_pipe, rag_retriever, prompt\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "  else:\n",
        "    st.stop()\n",
        "\n",
        "### BATCH PROCESSING ###\n",
        "st.markdown(\"---\")\n",
        "st.subheader(\"BATCH MODE\")\n",
        "\n",
        "if csv_file:\n",
        "    df = pd.read_csv(csv_file)\n",
        "    if st.button(\"Start Batch Processing\"):\n",
        "        results = []\n",
        "        futures = []\n",
        "        prog = st.progress(0)\n",
        "\n",
        "        BATCH_WORKERS = 1 if HF_DEVICE != \"cuda\" else 2\n",
        "        with ThreadPoolExecutor(max_workers=BATCH_WORKERS) as exe:\n",
        "            for txt in df[\"Case\"]:\n",
        "                futures.append(exe.submit(process_case, txt))\n",
        "\n",
        "            # as each case completes, update progress\n",
        "            for i, fut in enumerate(as_completed(futures)):\n",
        "                results.append(fut.result())\n",
        "                prog.progress((i + 1) / len(futures))\n",
        "\n",
        "        out_df = pd.DataFrame(results)\n",
        "        st.download_button(\n",
        "            \"Download Results as CSV\",\n",
        "            data=out_df.to_csv(index=False),\n",
        "            file_name=\"ddx_comparison.csv\"\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VpnZ57j8jEzI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8UdR9UqO6XCQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-g4_J0UfB6mQ"
      },
      "source": [
        "<h2>Install local-tunnel </h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73Wu2XhiCEiV",
        "outputId": "dd9ba217-4c2d-466c-88d7-692ea47def59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K\n",
            "added 22 packages in 3s\n",
            "\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K\n",
            "\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K3 packages are looking for funding\n",
            "\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K  run `npm fund` for details\n",
            "\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K"
          ]
        }
      ],
      "source": [
        "!npm install localtunnel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EPWz5NtCJnR"
      },
      "source": [
        "<h2> Run Streamlit in background </h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hAdTKrDCJM9",
        "outputId": "9a72480f-0bac-4377-a514-551d4d82db46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34.168.93.182\n",
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0Kyour url is: https://common-pianos-add.loca.lt\n"
          ]
        }
      ],
      "source": [
        "# AND Expose to the port 8501\n",
        "!streamlit run /content/app.py &>/content/logs.txt & npx localtunnel --port 8501 & curl ipv4.icanhazip.com"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CmWloMc4F7W"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
