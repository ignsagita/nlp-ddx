{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0hYelLS4K7V"
      },
      "source": [
        "<h1> Differential Diagnosis with Mistral 7B RAG vs. BioMistral 7B by ContactDoctor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ip-nf71A4HQb",
        "outputId": "7353de90-5ad1-4a0b-dd76-3cb3b81d1961"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "flask-oauthlib 0.9.6 requires oauthlib!=2.0.3,!=2.0.4,!=2.0.5,<3.0.0,>=1.1.2, but you have oauthlib 3.2.2 which is incompatible.\n",
            "flask-oauthlib 0.9.6 requires requests-oauthlib<1.2.0,>=0.6.2, but you have requests-oauthlib 2.0.0 which is incompatible.\n",
            "flet 0.7.4 requires httpx<0.24.0,>=0.23.3, but you have httpx 0.28.1 which is incompatible.\n",
            "flet 0.7.4 requires watchdog<3.0.0,>=2.2.1, but you have watchdog 4.0.1 which is incompatible.\n",
            "langchain-mistralai 0.2.10 requires langchain-core<1.0.0,>=0.3.49, but you have langchain-core 0.1.53 which is incompatible.\n",
            "\n",
            "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install -q streamlit langchain_community chromadb huggingface-hub bitsandbytes pypdf tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wPfBDSg4QUEO"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.makedirs('.streamlit', exist_ok=True)\n",
        "with open('.streamlit/secrets.toml', 'w') as f:\n",
        "    f.write(\"\"\"\n",
        "[huggingface]\n",
        "token = \"secret_token\"\n",
        "\n",
        "[models]\n",
        "rag = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "bio = \"BioMistral/BioMistral-7B\"\n",
        "\"\"\".lstrip())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94HxwELP4XyG",
        "outputId": "fd450a61-2a34-44c8-e274-ff8cbb7f989b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "\n",
        "# IMPORT LIBRARY\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import os\n",
        "import torch\n",
        "import re\n",
        "\n",
        "# FOR PARALLELIZATION\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain import PromptTemplate\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
        "import threading\n",
        "import time\n",
        "from tenacity import retry, stop_after_attempt, wait_fixed\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# CODE BLOCK\n",
        "\n",
        "PROMPT = \"\"\"Answer the question based only on the following context,:{context}\n",
        "Question:{question}\n",
        "What are the top 10 most likely diagnoses? Be precise, listing one diagnosis per line, and try to cover many unique possibilities.\n",
        "Ensure the order starts with the most likely. The top 10 diagnoses are.\"\"\"\n",
        "MAX_INPUT_TOKENS = 2048 # The sequence length limit of BioMistral-7V\n",
        "DB_DIR = \"./db_im\"\n",
        "\n",
        "HF_DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "PIPELINE_DEVICE = 0 if torch.cuda.is_available() else -1\n",
        "HF_TOKEN    = st.secrets[\"huggingface\"][\"token\"]\n",
        "model_id    = st.secrets[\"models\"][\"rag\"]\n",
        "bio_model_id= st.secrets[\"models\"][\"bio\"]\n",
        "\n",
        "\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "load_in_4bit=True,\n",
        "bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "bnb_4bit_use_double_quant=True,\n",
        "bnb_4bit_quant_type=\"nf4\",\n",
        ")\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    import os\n",
        "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "### HELPERS TO LOAD THE MODEL ###\n",
        "\n",
        "#lock to serialize any “move‐model‐on/off GPU” calls\n",
        "gpu_lock = threading.Lock()\n",
        "\n",
        "def unload_model_from_gpu(model):\n",
        "    \"\"\"Explicitly moves model to CPU and clears CUDA cache\"\"\"\n",
        "    if hasattr(model, \"to\"):\n",
        "        model.to(\"cpu\")\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "def safe_invoke(model_or_chain, *args, **kwargs):\n",
        "    try:\n",
        "        if hasattr(model_or_chain, \"invoke\"):\n",
        "            return model_or_chain.invoke(*args, **kwargs)\n",
        "        return model_or_chain(*args, **kwargs)\n",
        "\n",
        "    except Exception as e:\n",
        "        st.error(f\"MODEL ERROR: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def choose_specialty(current_case, pipe, prompt_specialty):\n",
        "    \"\"\" This function chooses the specialty that a medical case belongs to automatically.\n",
        "    It takes a model (model instance) and the case (str).\n",
        "    The model = Ollama(model=\"mistral\") was used in testing. It returns\n",
        "    the directory of the vector database as a string\"\"\"\n",
        "\n",
        "    prompt_specialty = prompt_specialty.format_prompt(current_case=current_case).to_string()\n",
        "\n",
        "    response_specialty = safe_invoke(pipe, prompt_specialty, max_new_tokens=128)\n",
        "\n",
        "    raw_specialty = response_specialty[0][\"generated_text\"]\n",
        "    # strip prompt echo\n",
        "    specialty_out = raw_specialty[len(prompt_specialty):].lstrip() if raw_specialty.startswith(prompt_specialty) else raw_specialty\n",
        "\n",
        "    db_dict = {'internal medicine': './db_im', 'obstetrics and gynecology': './db_og', 'pediatrics':'./db_p','surgery':'./db_surg','psychiatry':'./db_psy'}\n",
        "\n",
        "    result_specialty = re.sub(r'\\d\\.', '', specialty_out).strip().lower()\n",
        "    return db_dict[result_specialty]\n",
        "\n",
        "\n",
        "### CACHING HEAVY RESOURCES ###\n",
        "\n",
        "@st.cache_resource(show_spinner=False)\n",
        "def get_embedding_fn():\n",
        "  return HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\", model_kwargs = {\"device\": HF_DEVICE})\n",
        "\n",
        "\n",
        "\n",
        "# Load Mistral 7B RAG\n",
        "@st.cache_resource(show_spinner=False)\n",
        "def get_rag_components(txt):\n",
        "\n",
        "\n",
        "    mod = AutoModelForCausalLM.from_pretrained(model_id,\n",
        "                                               use_auth_token= HF_TOKEN,\n",
        "                                               device_map='auto' if HF_DEVICE==\"cuda\" else \"cpu\",\n",
        "                                               torch_dtype= torch.bfloat16 if HF_DEVICE==\"cuda\" else torch.float32,\n",
        "                                               quantization_config=bnb_config)\n",
        "\n",
        "    pipe  = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=mod,\n",
        "        tokenizer=AutoTokenizer.from_pretrained(model_id, use_auth_token=HF_TOKEN),\n",
        "        #device=\"cpu\",\n",
        "        use_fast=True,\n",
        "        max_new_tokens=4,\n",
        "    )\n",
        "\n",
        "    prompt = PromptTemplate(template=PROMPT, input_variables=[\"context\", \"question\"])\n",
        "    PROMPT_specialty_template = \"\"\"\"{current_case} What is the medical specialty of this case? Choose from this list 1. Internal Medicine, 2. Obstetrics and Gynecology, 3. Pediatrics, 4. Surgery 5. Psychiatry\\n\" \"\"\"\n",
        "\n",
        "\n",
        "    prompt_specialty = PromptTemplate(template=PROMPT_specialty_template, input_variables=[\"current_case\"])\n",
        "\n",
        "    DB_DIR = choose_specialty(txt, pipe, prompt_specialty)\n",
        "\n",
        "\n",
        "    vs = Chroma(\n",
        "    embedding_function=get_embedding_fn(),\n",
        "    persist_directory=DB_DIR,\n",
        "    )\n",
        "    pipe  = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=mod,\n",
        "        tokenizer=AutoTokenizer.from_pretrained(model_id, use_auth_token=HF_TOKEN),\n",
        "        #device=\"cpu\",\n",
        "        use_fast=True,\n",
        "        max_new_tokens=256,\n",
        "    )\n",
        "    retriever  = vs.as_retriever()\n",
        "    return pipe, retriever, prompt\n",
        "\n",
        "\n",
        "# Load Bio Model\n",
        "@st.cache_resource(show_spinner=False)\n",
        "def get_bio_pipeline():\n",
        "\n",
        "    bio_mod     = AutoModelForCausalLM.from_pretrained(bio_model_id,\n",
        "                                               use_auth_token= HF_TOKEN,\n",
        "                                               device_map='auto' if HF_DEVICE==\"cuda\" else \"cpu\",\n",
        "                                               torch_dtype= torch.bfloat16 if HF_DEVICE==\"cuda\" else torch.float32,\n",
        "                                               quantization_config=bnb_config)\n",
        "\n",
        "\n",
        "    bio_pipe  =  pipeline(\n",
        "        \"text-generation\",\n",
        "        model=bio_mod,\n",
        "        tokenizer=AutoTokenizer.from_pretrained(bio_model_id, use_auth_token=HF_TOKEN),\n",
        "        use_fast=True,\n",
        "        max_new_tokens=256,\n",
        "    )\n",
        "\n",
        "    return bio_pipe\n",
        "\n",
        "@st.cache_resource\n",
        "def get_tokenizer():\n",
        "    return AutoTokenizer.from_pretrained(model_id, use_auth_token=HF_TOKEN)\n",
        "\n",
        "\n",
        "# Tricks for Expensive File I/O: streamlit caching\n",
        "@st.cache_data(show_spinner=False)\n",
        "def build_vectorstore(uploaded_files, persist_directory):\n",
        "    folder_dict = {'./db_im': 'im_folder', './db_og': 'og_folder', './db_p':'p_folder','./db_surg':'surg_folder','./db_psy':'psy_folder'}\n",
        "\n",
        "    UploadedTextbook = folder_dict[persist_directory]\n",
        "\n",
        "    os.makedirs(UploadedTextbook, exist_ok=True)\n",
        "    paths = []\n",
        "    for f in uploaded_files:\n",
        "        path = os.path.join(UploadedTextbook, f.name)\n",
        "        with open(path, \"wb\") as fp:\n",
        "            fp.write(f.getbuffer())\n",
        "        paths.append(path)\n",
        "\n",
        "    docs = []\n",
        "    for pdf in paths:\n",
        "        docs.extend(PyPDFLoader(pdf).load())\n",
        "    splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
        "        chunk_size=1500, chunk_overlap=200\n",
        "    )\n",
        "    splits = splitter.split_documents(docs)\n",
        "\n",
        "    vs = Chroma.from_documents(\n",
        "        splits,\n",
        "        get_embedding_fn(),\n",
        "        persist_directory=persist_directory,\n",
        "    )\n",
        "    vs.persist()\n",
        "    return True\n",
        "\n",
        "### HELPERS ###\n",
        "\n",
        "# Make sure the token inputs are within the limit\n",
        "def check_length(text):\n",
        "    tok = get_tokenizer()\n",
        "    token_count = len(tok.encode(text))\n",
        "    if token_count > MAX_INPUT_TOKENS:\n",
        "        st.warning(f\"Your input is {token_count} tokens, over the {MAX_INPUT_TOKENS}-token limit. Please shorten it.\")\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "def safe_invoke(model_or_chain, *args, **kwargs):\n",
        "    try:\n",
        "        if hasattr(model_or_chain, \"invoke\"):\n",
        "            return model_or_chain.invoke(*args, **kwargs)\n",
        "        return model_or_chain(*args, **kwargs)\n",
        "\n",
        "    except Exception as e:\n",
        "        st.error(f\"MODEL ERROR: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def process_case(txt):\n",
        "    # 1) Retrieve context\n",
        "    try:\n",
        "      rag_pipe, rag_retriever, prompt = get_rag_components(txt)\n",
        "      docs    = rag_retriever.get_relevant_documents(txt)\n",
        "      context = \"\\n\\n\".join(d.page_content for d in docs)\n",
        "\n",
        "      # 2) Generate with RAG‐LLM\n",
        "      prompt_text = prompt.format_prompt(context=context, question=txt).to_string()\n",
        "\n",
        "      with gpu_lock:\n",
        "        rag_out_raw = safe_invoke(rag_pipe, prompt_text, max_new_tokens=128)\n",
        "        #clear cache\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "      if rag_out_raw:\n",
        "          raw = rag_out_raw[0][\"generated_text\"]\n",
        "          # strip prompt echo\n",
        "          rag_out = raw[len(prompt_text):].lstrip() if raw.startswith(prompt_text) else raw\n",
        "      else:\n",
        "          rag_out = \"Error\"\n",
        "\n",
        "      # 3) Generate with Bio‐LLM\n",
        "      bio_pipe    = get_bio_pipeline()\n",
        "      bio_prompt  = PROMPT.format(context=\"\", question=txt)\n",
        "      with gpu_lock:\n",
        "        bio_raw = safe_invoke(bio_pipe, bio_prompt, max_new_tokens=128)\n",
        "        #clear cache\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "      if bio_raw:\n",
        "          raw_b = bio_raw[0][\"generated_text\"]\n",
        "          bio_out = raw_b[len(bio_prompt):].lstrip() if raw_b.startswith(bio_prompt) else raw_b\n",
        "      else:\n",
        "          bio_out = \"Error\"\n",
        "\n",
        "      return {\"Case\": txt, \"Mistral7B+RAG\": rag_out, \"BioMistral7B\": bio_out}\n",
        "\n",
        "    except Exception as e:\n",
        "        return {\"Case\": txt, \"Mistral7B+RAG\": f\"ERROR: {e}\", \"BioMistral7B\": \"\"}\n",
        "\n",
        "# Then when generating, temporarily move to GPU:\n",
        "def generate_bio(bio_pipe, prompt):\n",
        "  with gpu_lock:\n",
        "    # move to GPU\n",
        "    bio_pipe.model.to(\"cuda\")\n",
        "    out = bio_pipe(prompt)[0][\"generated_text\"]\n",
        "    # move back to CPU to free VRAM\n",
        "    bio_pipe.model.to(\"cpu\")\n",
        "  return out\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### STREAMLIT UI ###\n",
        "st.title(\"Differential Diagnosis: Mistral 7B RAG vs. Bio Mistral 7B by BioMistral\")\n",
        "st.caption(\"Helps the doctor/nurse to develop their differential diagnosis using LLM models\")\n",
        "\n",
        "# Additional files\n",
        "with st.sidebar:\n",
        "    db_dict = {'internal medicine': './db_im', 'obstetrics and gynecology': './db_og', 'pediatrics':'./db_p','surgery':'./db_surg','psychiatry':'./db_psy'}\n",
        "    persist_directory = \"./db_im\"\n",
        "    specialty = st.selectbox(\"Choose Specialty\", ('internal medicine', 'obstetrics and gynecology', 'pediatrics','surgery','psychiatry'))\n",
        "    persist_directory = db_dict[specialty]\n",
        "    # if st.button(\"Set Specialty\"):\n",
        "    #     persist_directory = db_dict[specialty]\n",
        "    #     print(persist_directory)\n",
        "\n",
        "    st.header(\"Upload additional resources for RAG (type:.pdf)\")\n",
        "    UploadedFiles = st.file_uploader(\"Upload here and click on 'Upload'\", type=\"pdf\", accept_multiple_files=True)\n",
        "\n",
        "    if st.button(\"Build Index\"):\n",
        "        if not UploadedFiles:\n",
        "            st.error(\"Select at least one PDF first.\")\n",
        "        else:\n",
        "            with st.spinner(\"Indexing…\"):\n",
        "                build_vectorstore(UploadedFiles, persist_directory)\n",
        "            st.success(\"RAG index is ready!\")\n",
        "\n",
        "    st.markdown(\"---\")\n",
        "    st.header(\"Batch processing case upload (type:.csv)\")\n",
        "    csv_file = st.file_uploader(\n",
        "        \"Upload CSV\",\n",
        "        type=\"csv\",\n",
        "        accept_multiple_files=False)\n",
        "\n",
        "### SINGLE CASE ###\n",
        "st.subheader(\"SINGLE CASE\")\n",
        "question = st.text_area(\"Case Narrative:\",\n",
        "                        height=180,\n",
        "                        placeholder=\"For example: 22-year-old patient with TB was admitted to hospital today. The patient has been to a country outside Sweden. The patient came back to Sweden from the other country. The patient has had a fever for two weeks and is admitted. The doctor has prescribed a medicine. \")\n",
        "st.write(f\"The number of characters are {len(question)} characters.\")\n",
        "\n",
        "if st.button('Start Processing'):\n",
        "  if check_length(question):\n",
        "    with st.spinner(\"Processing...\"):\n",
        "\n",
        "      tabs = st.tabs([\"BIOMode\", \"RAGMode\"])\n",
        "\n",
        "      with tabs[0]:\n",
        "        #Biomodel execution\n",
        "        bio_pipe = get_bio_pipeline()\n",
        "        bio_prompt = PROMPT.format(context=\"\",question=question)\n",
        "\n",
        "        with gpu_lock:\n",
        "          raw_bio = bio_pipe(bio_prompt)[0][\"generated_text\"]\n",
        "          torch.cuda.empty_cache()\n",
        "\n",
        "        #remove the prompt echo\n",
        "        if raw_bio.startswith(bio_prompt):\n",
        "            bio_output = raw_bio[len(bio_prompt):].lstrip()\n",
        "        else:\n",
        "            bio_output = raw_bio\n",
        "\n",
        "        st.markdown(\"**BioMistral 7B**\")\n",
        "        st.text(bio_output)\n",
        "\n",
        "      with tabs[1]:\n",
        "        if not os.path.isdir(DB_DIR) or not os.listdir(DB_DIR):\n",
        "          st.error(\"Please upload and build your PDF index first!\")\n",
        "          st.stop()\n",
        "\n",
        "        rag_pipe, rag_retriever, prompt = get_rag_components(question)\n",
        "        docs = rag_retriever.get_relevant_documents(question)\n",
        "        context = \"\\n\\n\".join([d.page_content for d in docs])\n",
        "        prompt_text = prompt.format_prompt(context=context, question=question).to_string()\n",
        "\n",
        "        st.markdown(\"**Mistral 7B + RAG**\")\n",
        "\n",
        "        with gpu_lock:\n",
        "          raw = rag_pipe(prompt_text)[0][\"generated_text\"]\n",
        "          torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "        #remove the prompt echo\n",
        "        if raw.startswith(prompt_text):\n",
        "            answer = raw[len(prompt_text):].lstrip()\n",
        "        else:\n",
        "            answer = raw\n",
        "        st.text(answer)\n",
        "\n",
        "        #free up memory from RAG\n",
        "        del rag_pipe, rag_retriever, prompt\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "  else:\n",
        "    st.stop()\n",
        "\n",
        "### BATCH PROCESSING ###\n",
        "st.markdown(\"---\")\n",
        "st.subheader(\"BATCH MODE\")\n",
        "\n",
        "if csv_file:\n",
        "    df = pd.read_csv(csv_file)\n",
        "    if st.button(\"Start Batch Processing\"):\n",
        "        results = []\n",
        "        futures = []\n",
        "        prog = st.progress(0)\n",
        "\n",
        "        BATCH_WORKERS = 1 if HF_DEVICE != \"cuda\" else 2\n",
        "        with ThreadPoolExecutor(max_workers=BATCH_WORKERS) as exe:\n",
        "            for txt in df[\"Case\"]:\n",
        "                futures.append(exe.submit(process_case, txt))\n",
        "\n",
        "            # as each case completes, update progress\n",
        "            for i, fut in enumerate(as_completed(futures)):\n",
        "                results.append(fut.result())\n",
        "                prog.progress((i + 1) / len(futures))\n",
        "\n",
        "        out_df = pd.DataFrame(results)\n",
        "        st.download_button(\n",
        "            \"Download Results as CSV\",\n",
        "            data=out_df.to_csv(index=False),\n",
        "            file_name=\"ddx_comparison.csv\"\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VpnZ57j8jEzI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8UdR9UqO6XCQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-g4_J0UfB6mQ"
      },
      "source": [
        "<h2>Install local-tunnel </h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73Wu2XhiCEiV",
        "outputId": "2bd6cc62-b10d-4371-9a8b-4d237d5840d9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'npm' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "!npm install localtunnel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EPWz5NtCJnR"
      },
      "source": [
        "<h2> Run Streamlit in background </h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hAdTKrDCJM9",
        "outputId": "10cc7cda-b4bc-4530-aee1-9304dab22e4a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "& was unexpected at this time.\n"
          ]
        }
      ],
      "source": [
        "# AND Expose to the port 8501\n",
        "!streamlit run /content/app.py &>/content/logs.txt & npx localtunnel --port 8501 & curl ipv4.icanhazip.com"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CmWloMc4F7W"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
