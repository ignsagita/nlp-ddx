{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0hYelLS4K7V"
      },
      "source": [
        "<h1> Differential Diagnosis with Mistral 7B RAG vs. BioMistral 7B by ContactDoctor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ip-nf71A4HQb",
        "outputId": "ef431e8b-381f-47e8-d8ae-ee755649a980"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m136.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m105.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.9/18.9 MB\u001b[0m \u001b[31m107.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m303.4/303.4 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m79.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m87.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.9/194.9 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.9/118.9 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m95.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m128.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m89.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m62.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m105.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m108.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q streamlit langchain_community chromadb huggingface-hub bitsandbytes pypdf tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wPfBDSg4QUEO"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.makedirs('.streamlit', exist_ok=True)\n",
        "with open('.streamlit/secrets.toml', 'w') as f:\n",
        "    f.write(\"\"\"\n",
        "[huggingface]\n",
        "token = \"secret_token\"\n",
        "\n",
        "[models]\n",
        "rag = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "bio = \"BioMistral/BioMistral-7B\"\n",
        "\"\"\".lstrip())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94HxwELP4XyG",
        "outputId": "4f2403dd-bcb3-4aa7-da3a-541510631662"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "\n",
        "# IMPORT LIBRARY\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import os\n",
        "import torch\n",
        "import re\n",
        "\n",
        "\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed # FOR PARALLELIZATION\n",
        "\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain import PromptTemplate\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
        "import threading\n",
        "import time\n",
        "from tenacity import retry, stop_after_attempt, wait_fixed\n",
        "import gc\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# CODE BLOCK\n",
        "\n",
        "PROMPT = \"\"\"Answer the question based only on the following context,:{context}\n",
        "Question:{question}\n",
        "What are the top 10 most likely diagnoses? Be precise, listing one diagnosis per line, and try to cover many unique possibilities.\n",
        "Ensure the order starts with the most likely. The top 10 diagnoses are.\"\"\"\n",
        "MAX_INPUT_TOKENS = 2048 # The sequence length limit of BioMistral-7V\n",
        "MAX_CONTEXT_LENGTH = 4096 # Total context length including prompt\n",
        "DB_DIR = \"./db_im\"\n",
        "\n",
        "HF_DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "PIPELINE_DEVICE = 0 if torch.cuda.is_available() else -1\n",
        "HF_TOKEN    = st.secrets[\"huggingface\"][\"token\"]\n",
        "model_id    = st.secrets[\"models\"][\"rag\"]\n",
        "bio_model_id= st.secrets[\"models\"][\"bio\"]\n",
        "\n",
        "\n",
        "# using 4bit to save memory for model loading\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "load_in_4bit=True,\n",
        "bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "bnb_4bit_use_double_quant=True,\n",
        "bnb_4bit_quant_type=\"nf4\",\n",
        ")\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    import os\n",
        "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True, max_split_size_mb:512, garbage_collection_threshold:0.8\"\n",
        "\n",
        "\n",
        "### HELPERS TO LOAD THE MODEL ###\n",
        "\n",
        "#lock to serialize any “move‐model‐on/off GPU” calls\n",
        "gpu_lock = threading.Lock()\n",
        "\n",
        "\n",
        "\n",
        "def unload_model_from_gpu(model):\n",
        "    \"\"\"Explicitly moves model to CPU and clears CUDA cache\"\"\"\n",
        "    if hasattr(model, \"to\"):\n",
        "        model.to(\"cpu\")\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "def safe_invoke(model_or_chain, *args, **kwargs):\n",
        "    try:\n",
        "        if hasattr(model_or_chain, \"invoke\"):\n",
        "            return model_or_chain.invoke(*args, **kwargs)\n",
        "        return model_or_chain(*args, **kwargs)\n",
        "\n",
        "    except Exception as e:\n",
        "        st.error(f\"MODEL ERROR: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def choose_specialty(current_case, pipe, prompt_specialty):\n",
        "    \"\"\" This function chooses the specialty that a medical case belongs to automatically.\n",
        "    It takes a model (model instance) and the case (str).\n",
        "    The model = Ollama(model=\"mistral\") was used in testing. It returns\n",
        "    the directory of the vector database as a string\"\"\"\n",
        "\n",
        "    prompt_specialty = prompt_specialty.format_prompt(current_case=current_case).to_string()\n",
        "\n",
        "    response_specialty = safe_invoke(pipe, prompt_specialty, max_new_tokens=128)\n",
        "\n",
        "    raw_specialty = response_specialty[0][\"generated_text\"]\n",
        "    # strip prompt echo\n",
        "    specialty_out = raw_specialty[len(prompt_specialty):].lstrip() if raw_specialty.startswith(prompt_specialty) else raw_specialty\n",
        "\n",
        "    db_dict = {'internal medicine': './db_im', 'obstetrics and gynecology': './db_og', 'pediatrics':'./db_p','surgery':'./db_surg','psychiatry':'./db_psy'}\n",
        "\n",
        "    result_specialty = re.sub(r'\\d\\.', '', specialty_out).strip().lower()\n",
        "    specialty_only = re.split('\\.', result_specialty)[0]\n",
        "    try:\n",
        "      return db_dict[specialty_only]\n",
        "    except:\n",
        "      return './db_im'\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "@retry(stop=stop_after_attempt(3), wait=wait_fixed(2))\n",
        "def unload_model_from_gpu(model=None):\n",
        "    \"\"\"Clears CUDA cache and forces garbage collection to save memory\"\"\"\n",
        "    with gpu_lock:\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "\n",
        "### CACHING HEAVY RESOURCES ###\n",
        "\n",
        "@st.cache_resource(show_spinner=False)\n",
        "def get_embedding_fn():\n",
        "  return HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\", model_kwargs = {\"device\": HF_DEVICE})\n",
        "\n",
        "#load tokenizer separately for faster token counting\n",
        "@st.cache_resource\n",
        "def get_tokenizer():\n",
        "    return AutoTokenizer.from_pretrained(model_id, use_auth_token=HF_TOKEN)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#load Mistral 7B RAG\n",
        "@st.cache_resource(show_spinner=False)\n",
        "def get_rag_components(txt):\n",
        "\n",
        "\n",
        "    mod = AutoModelForCausalLM.from_pretrained(model_id,\n",
        "                                               use_auth_token= HF_TOKEN,\n",
        "                                               device_map='auto' if HF_DEVICE==\"cuda\" else \"cpu\",\n",
        "                                               torch_dtype= torch.bfloat16 if HF_DEVICE==\"cuda\" else torch.float32,\n",
        "                                               quantization_config=bnb_config)\n",
        "\n",
        "    choose_specialty_pipe  = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=mod,\n",
        "        tokenizer=AutoTokenizer.from_pretrained(model_id, use_auth_token=HF_TOKEN),\n",
        "        #device=\"cpu\",\n",
        "        use_fast=True,\n",
        "        max_new_tokens=4,\n",
        "    )\n",
        "\n",
        "    PROMPT_specialty_template = \"\"\"\"{current_case} What is the medical specialty of this case? Choose from this list 1. Internal Medicine, 2. Obstetrics and Gynecology, 3. Pediatrics, 4. Surgery 5. Psychiatry\\n\" \"\"\"\n",
        "\n",
        "\n",
        "    prompt_specialty = PromptTemplate(template=PROMPT_specialty_template, input_variables=[\"current_case\"])\n",
        "\n",
        "    DB_DIR = choose_specialty(txt, choose_specialty_pipe, prompt_specialty)\n",
        "\n",
        "\n",
        "    vs = Chroma(\n",
        "    embedding_function=get_embedding_fn(),\n",
        "    persist_directory=DB_DIR,\n",
        "    )\n",
        "    pipe  = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=mod,\n",
        "        tokenizer=AutoTokenizer.from_pretrained(model_id, use_auth_token=HF_TOKEN),\n",
        "        #device=\"cpu\",\n",
        "        use_fast=True,\n",
        "        max_new_tokens=256,\n",
        "    )\n",
        "    retriever  = vs.as_retriever()\n",
        "    prompt = PromptTemplate(template=PROMPT, input_variables=[\"context\", \"question\"])\n",
        "\n",
        "    return pipe, retriever, prompt\n",
        "\n",
        "#load Bio model\n",
        "@st.cache_resource(show_spinner=False)\n",
        "def get_bio_pipeline():\n",
        "  with gpu_lock:\n",
        "    #unload_model_from_gpu()\n",
        "\n",
        "    bio_mod     = AutoModelForCausalLM.from_pretrained(bio_model_id,\n",
        "                                               use_auth_token= HF_TOKEN,\n",
        "                                               device_map='auto' if HF_DEVICE==\"cuda\" else \"cpu\",\n",
        "                                               torch_dtype= torch.bfloat16 if HF_DEVICE==\"cuda\" else torch.float32,\n",
        "                                               quantization_config=bnb_config)\n",
        "\n",
        "\n",
        "    bio_pipe  =  pipeline(\n",
        "        \"text-generation\",\n",
        "        model=bio_mod,\n",
        "        tokenizer=AutoTokenizer.from_pretrained(bio_model_id, use_auth_token=HF_TOKEN),\n",
        "        use_fast=True,\n",
        "        max_new_tokens=256,\n",
        "    )\n",
        "\n",
        "    return bio_pipe\n",
        "\n",
        "\n",
        "#streamlit caching: tricks for expensive file I/O:\n",
        "@st.cache_data(show_spinner=False)\n",
        "def build_vectorstore(uploaded_files, DB_DIR):\n",
        "    folder_dict = {'./db_im': 'im_folder', './db_og': 'og_folder', './db_p':'p_folder','./db_surg':'surg_folder','./db_psy':'psy_folder'}\n",
        "\n",
        "    #start fresh every build index\n",
        "\n",
        "    # paths = []\n",
        "\n",
        "\n",
        "    UploadedTextbook = folder_dict[DB_DIR]\n",
        "\n",
        "    # if os.path.isdir(DB_DIR):\n",
        "    #     for fn in os.listdir(DB_DIR):\n",
        "    #         os.remove(os.path.join(DB_DIR, fn))\n",
        "    # os.makedirs(DB_DIR, exist_ok=True)\n",
        "    os.makedirs(UploadedTextbook, exist_ok=True)\n",
        "    paths = []\n",
        "\n",
        "\n",
        "    for f in uploaded_files:\n",
        "        path = os.path.join(UploadedTextbook, f.name)\n",
        "        with open(path, \"wb\") as fp:\n",
        "            fp.write(f.getbuffer())\n",
        "        paths.append(path)\n",
        "\n",
        "    docs = []\n",
        "    for pdf in paths:\n",
        "        docs.extend(PyPDFLoader(pdf).load())\n",
        "\n",
        "    splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
        "        chunk_size=1000, chunk_overlap=150 #define chunking strategy here: smaller for better targeting\n",
        "    )\n",
        "    splits = splitter.split_documents(docs)\n",
        "\n",
        "    vs = Chroma.from_documents(\n",
        "        splits,\n",
        "        get_embedding_fn(),\n",
        "        persist_directory=DB_DIR,\n",
        "    )\n",
        "    vs.persist()\n",
        "    return True\n",
        "\n",
        "### HELPERS ###\n",
        "\n",
        "#making sure the token inputs are within the limit\n",
        "def check_length(text, tokenizer=None):\n",
        "  if tokenizer is None:\n",
        "    tokenizer = get_tokenizer()\n",
        "  token_count = len(tokenizer.encode(text))\n",
        "  if token_count > MAX_INPUT_TOKENS:\n",
        "      st.warning(f\"Your input is {token_count} tokens, over the {MAX_INPUT_TOKENS}-token limit. Please shorten it.\")\n",
        "      return False\n",
        "  return True\n",
        "\n",
        "#check context length after RAG retrieval\n",
        "def check_context_length(prompt_text, tokenizer=None):\n",
        "  if not tokenizer:\n",
        "      tokenizer = get_tokenizer()\n",
        "  token_count = len(tokenizer.encode(prompt_text))\n",
        "  if token_count > MAX_CONTEXT_LENGTH-256:\n",
        "      st.warning(f\"Total context + prompt is {token_count} tokens, over the {MAX_CONTEXT_LENGTH}-token limit.\")\n",
        "      return False\n",
        "  return True\n",
        "\n",
        "\n",
        "def safe_invoke(model_or_chain, *args, **kwargs):\n",
        "    try:\n",
        "        if hasattr(model_or_chain, \"invoke\"):\n",
        "            return model_or_chain.invoke(*args, **kwargs)\n",
        "        return model_or_chain(*args, **kwargs)\n",
        "\n",
        "    except Exception as e:\n",
        "        st.error(f\"MODEL ERROR: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "\n",
        "#error handling for bio and naive model\n",
        "@retry(stop=stop_after_attempt(2), wait=wait_fixed(2))\n",
        "def run_rag_pipeline(pipe, prompt_text):\n",
        "    try:\n",
        "        raw_output = safe_invoke(pipe, prompt_text, max_new_tokens=256)\n",
        "        with gpu_lock:\n",
        "            pipe.model.to(\"cpu\")\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        if raw_output:\n",
        "            raw = raw_output[0][\"generated_text\"]\n",
        "            #strip prompt echo to avoid prompt redudancy\n",
        "            output = raw[len(prompt_text):].lstrip() if raw.startswith(prompt_text) else raw\n",
        "            return output\n",
        "\n",
        "        unload_model_from_gpu()\n",
        "        return \"Error: No output generated\"\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\"\n",
        "\n",
        "\n",
        "@retry(stop=stop_after_attempt(2), wait=wait_fixed(2))\n",
        "def run_bio_pipeline(pipe, prompt_text):\n",
        "    try:\n",
        "        raw_output = safe_invoke(pipe, prompt_text, max_new_tokens=256)\n",
        "        with gpu_lock:\n",
        "            pipe.model.to(\"cpu\")\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        if raw_output:\n",
        "            raw = raw_output[0][\"generated_text\"]\n",
        "            #strip prompt echo to avoid redudancy\n",
        "            output = raw[len(prompt_text):].lstrip() if raw.startswith(prompt_text) else raw\n",
        "            return output\n",
        "        return \"Error: No output generated\"\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\"\n",
        "\n",
        "\n",
        "\n",
        "#case processing\n",
        "def process_case(txt, use_rag=True, use_bio=True):\n",
        "    results = {\"Case\": txt}\n",
        "    tokenizer = get_tokenizer()\n",
        "\n",
        "    #validate input length first\n",
        "    if not check_length(txt, tokenizer):\n",
        "        results[\"Mistral7B+RAG\"] = \"Case too long, exceeds token limit\"\n",
        "        results[\"BioMistral7B\"] = \"Case too long, exceeds token limit\"\n",
        "        return results\n",
        "\n",
        "    # 1) Retrieve context\n",
        "    if use_rag:\n",
        "      try:\n",
        "        rag_pipe, rag_retriever, prompt = get_rag_components(txt)\n",
        "        docs    = rag_retriever.get_relevant_documents(txt)\n",
        "        context = \"\\n\\n\".join(d.page_content for d in docs)\n",
        "\n",
        "        # 2) Generate with RAG‐LLM\n",
        "        prompt_text = prompt.format_prompt(context=context, question=txt).to_string()\n",
        "\n",
        "        # Check combined length\n",
        "        if not check_context_length(prompt_text, tokenizer):\n",
        "            context_shortened = context[:len(context)//2] + \"...\"  #simple truncation\n",
        "            prompt_text = prompt.format_prompt(context=context_shortened, question=txt).to_string()\n",
        "\n",
        "        rag_out = run_rag_pipeline(rag_pipe, prompt_text)\n",
        "        results[\"Mistral7B+RAG\"] = rag_out\n",
        "\n",
        "        #free up GPU memory after RAG\n",
        "        unload_model_from_gpu()\n",
        "\n",
        "      except Exception as e:\n",
        "          results[\"Mistral7B+RAG\"] = f\"ERROR: {str(e)}\"\n",
        "    else:\n",
        "        results[\"Mistral7B+RAG\"] = \"RAG processing skipped\"\n",
        "\n",
        "    # 3) Generate with Bio‐LLM\n",
        "    if use_bio:\n",
        "      try:\n",
        "        bio_pipe    = get_bio_pipeline()\n",
        "        bio_prompt  = PROMPT.format(context=\"\", question=txt)\n",
        "        bio_out = run_bio_pipeline(bio_pipe, bio_prompt)\n",
        "        results[\"BioMistral7B\"] = bio_out\n",
        "\n",
        "        #free up GPU memory after bio_model\n",
        "        unload_model_from_gpu()\n",
        "\n",
        "      except Exception as e:\n",
        "            results[\"BioMistral7B\"] = f\"ERROR: {str(e)}\"\n",
        "    else:\n",
        "        results[\"BioMistral7B\"] = \"Bio processing skipped\"\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### STREAMLIT UI ###\n",
        "st.title(\"Differential Diagnosis: Mistral 7B RAG vs. Bio Mistral 7B by BioMistral\")\n",
        "st.caption(\"Helps the doctor/nurse to develop their differential diagnosis using LLM models\")\n",
        "\n",
        "# Additional files\n",
        "with st.sidebar:\n",
        "    db_dict = {'internal medicine': './db_im', 'obstetrics and gynecology': './db_og', 'pediatrics':'./db_p','surgery':'./db_surg','psychiatry':'./db_psy'}\n",
        "    DB_DIR = \"./db_im\"\n",
        "    specialty = st.selectbox(\"Choose Specialty\", ('internal medicine', 'obstetrics and gynecology', 'pediatrics','surgery','psychiatry'))\n",
        "    DB_DIR = db_dict[specialty]\n",
        "    st.header(\"Upload additional resources for RAG (type:.pdf)\")\n",
        "    UploadedFiles = st.file_uploader(\"Upload here and click on 'Upload'\", type=\"pdf\", accept_multiple_files=True)\n",
        "    MAX_LINES = 3 # limit maximum document uploaded\n",
        "    if len(UploadedFiles) > MAX_LINES:\n",
        "      st.warning(f\"Maximum number of files reached. Only the first {MAX_LINES} will be processed.\")\n",
        "      UploadedFiles = UploadedFiles[:MAX_LINES]\n",
        "\n",
        "\n",
        "    if st.button(\"Build Index\"):\n",
        "        if not UploadedFiles:\n",
        "            st.error(\"Select at least one PDF first.\")\n",
        "        else:\n",
        "            with st.spinner(\"Indexing…\"):\n",
        "                build_vectorstore(UploadedFiles, DB_DIR)\n",
        "            st.success(\"RAG index is ready!\")\n",
        "\n",
        "    st.markdown(\"---\")\n",
        "    st.header(\"Batch processing case upload (type:.xslx)\")\n",
        "    csv_file = st.file_uploader(\n",
        "        \"Upload Excel\",\n",
        "        type=\"xlsx\",\n",
        "        accept_multiple_files=False)\n",
        "\n",
        "### SINGLE CASE ###\n",
        "st.subheader(\"SINGLE CASE\")\n",
        "question = st.text_area(\"Case Narrative:\",\n",
        "                        height=180,\n",
        "                        placeholder=\"For example: 22-year-old patient with TB was admitted to hospital today. The patient has been to a country outside Sweden. The patient came back to Sweden from the other country. The patient has had a fever for two weeks and is admitted. The doctor has prescribed a medicine. \")\n",
        "st.write(f\"The number of characters are {len(question)} characters.\")\n",
        "\n",
        "if st.button('Start Processing'):\n",
        "  if check_length(question):\n",
        "    with st.spinner(\"Processing...\"):\n",
        "      result = process_case(question)\n",
        "      tabs = st.tabs([\"BIOMode\", \"RAGMode\"])\n",
        "\n",
        "      with tabs[0]:\n",
        "        #Biomodel execution\n",
        "        st.markdown(\"**BioMistral 7B**\")\n",
        "        st.text(result['BioMistral7B'])\n",
        "\n",
        "      with tabs[1]:\n",
        "        if not os.path.isdir(DB_DIR) or not os.listdir(DB_DIR):\n",
        "          st.error(\"Please upload and build your PDF index first!\")\n",
        "          st.stop()\n",
        "        else:\n",
        "          st.markdown(\"**Mistral 7B + RAG**\")\n",
        "          st.text(result['Mistral7B+RAG'])\n",
        "\n",
        "        #free up memory from bio\n",
        "        unload_model_from_gpu()\n",
        "\n",
        "  else:\n",
        "    st.stop()\n",
        "\n",
        "### BATCH PROCESSING ###\n",
        "st.markdown(\"---\")\n",
        "st.subheader(\"BATCH MODE\")\n",
        "\n",
        "if csv_file:\n",
        "    df = pd.read_excel(csv_file)\n",
        "    if st.button(\"Start Batch Processing\"):\n",
        "      with st.spinner(\"Processing...\"):\n",
        "        results = []\n",
        "        futures = []\n",
        "        prog = st.progress(0)\n",
        "\n",
        "        #only run one case at a time to avoid GPU memory constraints\n",
        "        BATCH_WORKERS = 1 # if HF_DEVICE != \"cuda\" else 2\n",
        "        with ThreadPoolExecutor(max_workers=BATCH_WORKERS) as exe:\n",
        "            for txt in df[\"Case\"]:\n",
        "                futures.append(exe.submit(process_case, txt))\n",
        "\n",
        "            # as each case completes, update progress\n",
        "            for i, fut in enumerate(as_completed(futures)):\n",
        "              try:\n",
        "                results.append(fut.result())\n",
        "              except Exception as e:\n",
        "                results.append({\"Case\": txt, \"Mistral7B+RAG\": f\"ERROR: {str(e)}\", \"BioMistral7B\": f\"ERROR: {str(e)}\"})\n",
        "              prog.progress((i + 1) / len(futures))\n",
        "\n",
        "        out_df = pd.DataFrame(results)\n",
        "        st.download_button(\n",
        "            \"Download Results as CSV\",\n",
        "            data=out_df.to_csv(index=False),\n",
        "            file_name=\"ddx_comparison.csv\"\n",
        "        )\n",
        "\n",
        "        # Show sample of results\n",
        "        st.write(\"Sample of processed results:\")\n",
        "        st.dataframe(out_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-g4_J0UfB6mQ"
      },
      "source": [
        "<h2>Install local-tunnel </h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73Wu2XhiCEiV",
        "outputId": "c31ae405-35eb-4c3a-e761-dec7b43a1ebf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K\n",
            "up to date, audited 23 packages in 1s\n",
            "\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K\n",
            "\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K3 packages are looking for funding\n",
            "\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K  run `npm fund` for details\n",
            "\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K\n",
            "2 \u001b[31m\u001b[1mhigh\u001b[22m\u001b[39m severity vulnerabilities\n",
            "\n",
            "To address all issues (including breaking changes), run:\n",
            "  npm audit fix --force\n",
            "\n",
            "Run `npm audit` for details.\n",
            "\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K"
          ]
        }
      ],
      "source": [
        "!npm install localtunnel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EPWz5NtCJnR"
      },
      "source": [
        "<h2> Run Streamlit in background </h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hAdTKrDCJM9",
        "outputId": "c5eb8932-1b05-44d2-e856-4f3ff8661cbc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "35.198.240.48\n",
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0Kyour url is: https://two-spies-punch.loca.lt\n"
          ]
        }
      ],
      "source": [
        "# AND Expose to the port 8501\n",
        "!streamlit run /content/app.py &>/content/logs.txt & npx localtunnel --port 8501 & curl ipv4.icanhazip.com"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CmWloMc4F7W"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
